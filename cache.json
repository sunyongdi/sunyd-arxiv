{"2023-11-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.09387v2","updated":"2023-11-21T18:31:57Z","published":"2023-11-15T21:30:26Z","title":"Banach-Tarski Embeddings and Transformers","summary":"  We introduce a new construction of embeddings of arbitrary recursive data\nstructures into high dimensional vectors. These embeddings provide an\ninterpretable model for the latent state vectors of transformers. We\ndemonstrate that these embeddings can be decoded to the original data structure\nwhen the embedding dimension is sufficiently large. This decoding algorithm has\na natural implementation as a transformer. We also show that these embedding\nvectors can be manipulated directly to perform computations on the underlying\ndata without decoding. As an example we present an algorithm that constructs\nthe embedded parse tree of an embedded token sequence using only vector\noperations in embedding space.\n","authors":["Joshua Maher"],"pdf_url":"https://arxiv.org/pdf/2311.09387v2.pdf","comment":"22 pages, 7 figures. v2: Fixed order of matrix multiplication in\n  section 2.4"},{"id":"http://arxiv.org/abs/2310.02168v2","updated":"2023-11-21T18:18:49Z","published":"2023-10-03T16:02:36Z","title":"Editing Personality for LLMs","summary":"  This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct a new benchmark dataset PersonalityEdit to address this task. Drawing\non the theory in Social Psychology, we isolate three representative traits,\nnamely Neuroticism, Extraversion, and Agreeableness, as the foundation for our\nbenchmark. We then gather data using GPT-4, generating responses that not only\nalign with a specified topic but also embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our intriguing findings uncover\npotential challenges of the proposed task, illustrating several remaining\nissues. We anticipate that our work can provide the NLP community with\ninsights. Code and datasets will be released at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Shengyu Mao","Ningyu Zhang","Xiaohan Wang","Mengru Wang","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02168v2.pdf","comment":"Work in progress, add more experiments"},{"id":"http://arxiv.org/abs/2310.02129v2","updated":"2023-11-21T17:59:04Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code is available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v2.pdf","comment":"Work in progress, add more experiments"},{"id":"http://arxiv.org/abs/2311.12735v1","updated":"2023-11-21T17:21:15Z","published":"2023-11-21T17:21:15Z","title":"LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource\n  Sentiment Analysis of Bangla Language","summary":"  This paper describes the system of the LowResource Team for Task 2 of\nBLP-2023, which involves conducting sentiment analysis on a dataset composed of\npublic posts and comments from diverse social media platforms. Our primary aim\nis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,\nusing various strategies including fine-tuning, dropping random tokens, and\nusing several external datasets. Our final model is an ensemble of the three\nbest BanglaBert variations. Our system has achieved overall 3rd in the Test Set\namong 30 participating teams with a score of 0.718. Additionally, we discuss\nthe promising systems that didn't perform well namely task-adaptive pertaining\nand paraphrasing using BanglaT5. Training codes and external datasets which are\nused for our system are publicly available at\nhttps://github.com/Aunabil4602/bnlp-workshop-task2-2023\n","authors":["Aunabil Chakma","Masum Hasan"],"pdf_url":"https://arxiv.org/pdf/2311.12735v1.pdf","comment":"Accepted at BLP Workshop @EMNLP2023"},{"id":"http://arxiv.org/abs/2311.12727v1","updated":"2023-11-21T17:03:21Z","published":"2023-11-21T17:03:21Z","title":"Soft Random Sampling: A Theoretical and Empirical Analysis","summary":"  Soft random sampling (SRS) is a simple yet effective approach for efficient\ntraining of large-scale deep neural networks when dealing with massive data.\nSRS selects a subset uniformly at random with replacement from the full data\nset in each epoch. In this paper, we conduct a theoretical and empirical\nanalysis of SRS. First, we analyze its sampling dynamics including data\ncoverage and occupancy. Next, we investigate its convergence with non-convex\nobjective functions and give the convergence rate. Finally, we provide its\ngeneralization performance. We empirically evaluate SRS for image recognition\non CIFAR10 and automatic speech recognition on Librispeech and an in-house\npayload dataset to demonstrate its effectiveness. Compared to existing\ncoreset-based data selection methods, SRS offers a better accuracy-efficiency\ntrade-off. Especially on real-world industrial scale data sets, it is shown to\nbe a powerful training strategy with significant speedup and competitive\nperformance with almost no additional computing cost.\n","authors":["Xiaodong Cui","Ashish Mittal","Songtao Lu","Wei Zhang","George Saon","Brian Kingsbury"],"pdf_url":"https://arxiv.org/pdf/2311.12727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10852v6","updated":"2023-11-21T16:36:43Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Feiyu Xiong","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.10852v6.pdf","comment":"Neurocomputing 2023"},{"id":"http://arxiv.org/abs/2306.17103v3","updated":"2023-11-21T16:32:41Z","published":"2023-06-29T17:01:51Z","title":"LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by\n  Whispering to ChatGPT","summary":"  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic\nlyrics transcription method achieving state-of-the-art performance on various\nlyrics transcription datasets, even in challenging genres such as rock and\nmetal. Our novel, training-free approach utilizes Whisper, a weakly supervised\nrobust speech recognition model, and GPT-4, today's most performant chat-based\nlarge language model. In the proposed method, Whisper functions as the \"ear\" by\ntranscribing the audio, while GPT-4 serves as the \"brain,\" acting as an\nannotator with a strong performance for contextualized output selection and\ncorrection. Our experiments show that LyricWhiz significantly reduces Word\nError Rate compared to existing methods in English and can effectively\ntranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz to\ncreate the first publicly available, large-scale, multilingual lyrics\ntranscription dataset with a CC-BY-NC-SA copyright license, based on\nMTG-Jamendo, and offer a human-annotated subset for noise level estimation and\nevaluation. We anticipate that our proposed method and dataset will advance the\ndevelopment of multilingual lyrics transcription, a challenging and emerging\ntask.\n","authors":["Le Zhuo","Ruibin Yuan","Jiahao Pan","Yinghao Ma","Yizhi LI","Ge Zhang","Si Liu","Roger Dannenberg","Jie Fu","Chenghua Lin","Emmanouil Benetos","Wenhu Chen","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2306.17103v3.pdf","comment":"9 pages, 2 figures, 5 tables, accepted by ISMIR 2023"},{"id":"http://arxiv.org/abs/2311.12707v1","updated":"2023-11-21T16:20:49Z","published":"2023-11-21T16:20:49Z","title":"Keeping Users Engaged During Repeated Administration of the Same\n  Questionnaire: Using Large Language Models to Reliably Diversify Questions","summary":"  Standardized, validated questionnaires are vital tools in HCI research and\nhealthcare, offering dependable self-report data. However, their repeated use\nin longitudinal or pre-post studies can induce respondent fatigue, impacting\ndata quality via response biases and decreased response rates. We propose\nutilizing large language models (LLMs) to generate diverse questionnaire\nversions while retaining good psychometric properties. In a longitudinal study,\nparticipants engaged with our agent system and responded daily for two weeks to\neither a standardized depression questionnaire or one of two LLM-generated\nquestionnaire variants, alongside a validated depression questionnaire.\nPsychometric testing revealed consistent covariation between the external\ncriterion and the focal measure administered across the three conditions,\ndemonstrating the reliability and validity of the LLM-generated variants.\nParticipants found the repeated administration of the standardized\nquestionnaire significantly more repetitive compared to the variants. Our\nfindings highlight the potential of LLM-generated variants to invigorate\nquestionnaires, fostering engagement and interest without compromising\nvalidity.\n","authors":["Hye Sun Yun","Mehdi Arjmand","Phillip Raymond Sherlock","Michael Paasche-Orlow","James W. Griffith","Timothy Bickmore"],"pdf_url":"https://arxiv.org/pdf/2311.12707v1.pdf","comment":"22 pages, preprint"},{"id":"http://arxiv.org/abs/2311.12699v1","updated":"2023-11-21T16:03:51Z","published":"2023-11-21T16:03:51Z","title":"Can Large Language Models Understand Content and Propagation for\n  Misinformation Detection: An Empirical Study","summary":"  Large Language Models (LLMs) have garnered significant attention for their\npowerful ability in natural language understanding and reasoning. In this\npaper, we present a comprehensive empirical study to explore the performance of\nLLMs on misinformation detection tasks. This study stands as the pioneering\ninvestigation into the understanding capabilities of multiple LLMs regarding\nboth content and propagation across social media platforms. Our empirical\nstudies on five misinformation detection datasets show that LLMs with diverse\nprompts achieve comparable performance in text-based misinformation detection\nbut exhibit notably constrained capabilities in comprehending propagation\nstructure compared to existing models in propagation-based misinformation\ndetection. Besides, we further design four instruction-tuned strategies to\nenhance LLMs for both content and propagation-based misinformation detection.\nThese strategies boost LLMs to actively learn effective features from multiple\ninstances or hard instances, and eliminate irrelevant propagation structures,\nthereby achieving better detection performance. Extensive experiments further\ndemonstrate LLMs would play a better capacity in content and propagation\nstructure under these proposed strategies and achieve promising detection\nperformance. These findings highlight the potential ability of LLMs to detect\nmisinformation.\n","authors":["Mengyang Chen","Lingwei Wei","Han Cao","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2311.12699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12689v1","updated":"2023-11-21T15:51:06Z","published":"2023-11-21T15:51:06Z","title":"Fair Text Classification with Wasserstein Independence","summary":"  Group fairness is a central research topic in text classification, where\nreaching fair treatment between sensitive groups (e.g. women vs. men) remains\nan open challenge. This paper presents a novel method for mitigating biases in\nneural text classification, agnostic to the model architecture. Considering the\ndifficulty to distinguish fair from unfair information in a text encoder, we\ntake inspiration from adversarial training to induce Wasserstein independence\nbetween representations learned to predict our target label and the ones\nlearned to predict some sensitive attribute. Our approach provides two\nsignificant advantages. Firstly, it does not require annotations of sensitive\nattributes in both testing and training data. This is more suitable for\nreal-life scenarios compared to existing methods that require annotations of\nsensitive attributes at train time. Second, our approach exhibits a comparable\nor better fairness-accuracy trade-off compared to existing methods.\n","authors":["Thibaud Leteno","Antoine Gourru","Charlotte Laclau","Rémi Emonet","Christophe Gravier"],"pdf_url":"https://arxiv.org/pdf/2311.12689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.12311v3","updated":"2023-11-21T15:40:54Z","published":"2020-12-22T19:32:52Z","title":"Influencer Videos: Unboxing the Mystique","summary":"  Influencer marketing has become a very popular tool to reach customers.\nDespite the rapid growth in influencer videos, there has been little research\non the effectiveness of their constituent features in explaining video\nengagement. We study YouTube influencers and analyze their unstructured video\ndata across text, audio and images using an \"interpretable deep learning\"\nframework that accomplishes both goals of prediction and interpretation. Our\nprediction-based approach analyzes unstructured data and finds that \"what is\nsaid\" in words (text) is more influential than \"how it is said\" in imagery\n(images) or acoustics (audio). Our novel interpretation-based approach is\nimplemented after completion of model prediction by analyzing the same source\nof unstructured data to measure importance attributed to the video features. We\neliminate several spurious relationships in two steps, identifying a subset of\nrelationships which are confirmed using theory. We uncover novel findings that\nestablish distinct associations for measures of shallow and deep engagement\nbased on the dual-system framework of human thinking. Our approach is validated\nusing simulated data, and we discuss the learnings from our findings for\ninfluencers and brands.\n","authors":["Prashant Rajaram","Puneet Manchanda"],"pdf_url":"https://arxiv.org/pdf/2012.12311v3.pdf","comment":"45 pages, Online Appendix"},{"id":"http://arxiv.org/abs/2311.12664v1","updated":"2023-11-21T15:14:54Z","published":"2023-11-21T15:14:54Z","title":"The DURel Annotation Tool: Human and Computational Measurement of\n  Semantic Proximity, Sense Clusters and Semantic Change","summary":"  We present the DURel tool that implements the annotation of semantic\nproximity between uses of words into an online, open source interface. The tool\nsupports standardized human annotation as well as computational annotation,\nbuilding on recent advances with Word-in-Context models. Annotator judgments\nare clustered with automatic graph clustering techniques and visualized for\nanalysis. This allows to measure word senses with simple and intuitive\nmicro-task judgments between use pairs, requiring minimal preparation efforts.\nThe tool offers additional functionalities to compare the agreement between\nannotators to guarantee the inter-subjectivity of the obtained judgments and to\ncalculate summary statistics giving insights into sense frequency\ndistributions, semantic variation or changes of senses over time.\n","authors":["Dominik Schlechtweg","Shafqat Mumtaz Virk","Pauline Sander","Emma Sköldberg","Lukas Theuer Linke","Tuo Zhang","Nina Tahmasebi","Jonas Kuhn","Sabine Schulte im Walde"],"pdf_url":"https://arxiv.org/pdf/2311.12664v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2311.12649v1","updated":"2023-11-21T14:49:00Z","published":"2023-11-21T14:49:00Z","title":"MathGloss: Building mathematical glossaries from text","summary":"  MathGloss is a project to create a knowledge graph (KG) for undergraduate\nmathematics from text, automatically, using modern natural language processing\n(NLP) tools and resources already available on the web. MathGloss is a linked\ndatabase of undergraduate concepts in mathematics. So far, it combines five\nresources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph\nhosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses\nat the University of Chicago, (iii) the syllabus of the French undergraduate\nmathematics curriculum which includes hyperlinks to the automated theorem\nprover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by\nmathematicians, and (v) the nLab, a wiki for category theory also curated by\nmathematicians. MathGloss's goal is to bring together resources for learning\nmathematics and to allow every mathematician to tailor their learning to their\nown preferences. Moreover, by organizing different resources for learning\nundergraduate mathematics alongside those for learning formal mathematics, we\nhope to make it easier for mathematicians and formal tools (theorem provers,\ncomputer algebra systems, etc) experts to \"understand\" each other and break\ndown some of the barriers to formal math.\n","authors":["Lucy Horowitz","Valeria de Paiva"],"pdf_url":"https://arxiv.org/pdf/2311.12649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01446v3","updated":"2023-11-21T14:02:33Z","published":"2023-09-04T08:54:20Z","title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models","summary":"  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n","authors":["Raz Lapid","Ron Langberg","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2309.01446v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12574v1","updated":"2023-11-21T12:40:01Z","published":"2023-11-21T12:40:01Z","title":"IMGTB: A Framework for Machine-Generated Text Detection Benchmarking","summary":"  In the era of large language models generating high quality texts, it is a\nnecessity to develop methods for detection of machine-generated text to avoid\nharmful use or simply due to annotation purposes. It is, however, also\nimportant to properly evaluate and compare such developed methods. Recently, a\nfew benchmarks have been proposed for this purpose; however, integration of\nnewest detection methods is rather challenging, since new methods appear each\nmonth and provide slightly different evaluation pipelines. In this paper, we\npresent the IMGTB framework, which simplifies the benchmarking of\nmachine-generated text detection methods by easy integration of custom (new)\nmethods and evaluation datasets. Its configurability and flexibility makes\nresearch and development of new detection methods easier, especially their\ncomparison to the existing state-of-the-art detectors. The default set of\nanalyses, metrics and visualizations offered by the tool follows the\nestablished practices of machine-generated text detection benchmarking found in\nstate-of-the-art literature.\n","authors":["Michal Spiegel","Dominik Macko"],"pdf_url":"https://arxiv.org/pdf/2311.12574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12538v1","updated":"2023-11-21T11:33:03Z","published":"2023-11-21T11:33:03Z","title":"In-Context Learning Functions with Varying Number of Minima","summary":"  Large Language Models (LLMs) have proven effective at In-Context Learning\n(ICL), an ability that allows them to create predictors from labeled examples.\nFew studies have explored the interplay between ICL and specific properties of\nfunctions it attempts to approximate. In our study, we use a formal framework\nto explore ICL and propose a new task of approximating functions with varying\nnumber of minima. We implement a method that allows for producing functions\nwith given inputs as minima. We find that increasing the number of minima\ndegrades ICL performance. At the same time, our evaluation shows that ICL\noutperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster\nthan 2NN in all settings. We validate the findings through a set of few-shot\nexperiments across various hyperparameter configurations.\n","authors":["David Oniani","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2311.12538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12537v1","updated":"2023-11-21T11:32:23Z","published":"2023-11-21T11:32:23Z","title":"Oasis: Data Curation and Assessment System for Pretraining of Large\n  Language Models","summary":"  Data is one of the most critical elements in building a large language model.\nHowever, existing systems either fail to customize a corpus curation pipeline\nor neglect to leverage comprehensive corpus assessment for iterative\noptimization of the curation. To this end, we present a pretraining corpus\ncuration and assessment platform called Oasis -- a one-stop system for data\nquality improvement and quantification with user-friendly interactive\ninterfaces. Specifically, the interactive modular rule filter module can devise\ncustomized rules according to explicit feedback. The debiased neural filter\nmodule builds the quality classification dataset in a negative-centric manner\nto remove the undesired bias. The adaptive document deduplication module could\nexecute large-scale deduplication with limited memory resources. These three\nparts constitute the customized data curation module. And in the holistic data\nassessment module, a corpus can be assessed in local and global views, with\nthree evaluation means including human, GPT-4, and heuristic metrics. We\nexhibit a complete process to use Oasis for the curation and assessment of\npretraining data. In addition, an 800GB bilingual corpus curated by Oasis is\npublicly released.\n","authors":["Tong Zhou","Yubo Chen","Pengfei Cao","Kang Liu","Jun Zhao","Shengping Liu"],"pdf_url":"https://arxiv.org/pdf/2311.12537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05140v2","updated":"2023-11-21T11:28:24Z","published":"2023-10-08T12:21:24Z","title":"Harnessing the Power of Large Language Models for Empathetic Response\n  Generation: Empirical Investigations and Improvements","summary":"  Empathetic dialogue is an indispensable part of building harmonious social\nrelationships and contributes to the development of a helpful AI. Previous\napproaches are mainly based on fine small-scale language models. With the\nadvent of ChatGPT, the application effect of large language models (LLMs) in\nthis field has attracted great attention. This work empirically investigates\nthe performance of LLMs in generating empathetic responses and proposes three\nimprovement methods of semantically similar in-context learning, two-stage\ninteractive generation, and combination with the knowledge base. Extensive\nexperiments show that LLMs can significantly benefit from our proposed methods\nand is able to achieve state-of-the-art performance in both automatic and human\nevaluations. Additionally, we explore the possibility of GPT-4 simulating human\nevaluators.\n","authors":["Yushan Qian","Wei-Nan Zhang","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05140v2.pdf","comment":"the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.12534v1","updated":"2023-11-21T11:26:26Z","published":"2023-11-21T11:26:26Z","title":"Evaluation Metrics of Language Generation Models for Synthetic Traffic\n  Generation Tasks","summary":"  Many Natural Language Generation (NLG) tasks aim to generate a single output\ntext given an input prompt. Other settings require the generation of multiple\ntexts, e.g., for Synthetic Traffic Generation (STG). This generation task is\ncrucial for training and evaluating QA systems as well as conversational\nagents, where the goal is to generate multiple questions or utterances\nresembling the linguistic variability of real users. In this paper, we show\nthat common NLG metrics, like BLEU, are not suitable for evaluating STG. We\npropose and evaluate several metrics designed to compare the generated traffic\nto the distribution of real user texts. We validate our metrics with an\nautomatic procedure to verify whether they capture different types of quality\nissues of generated data; we also run human annotations to verify the\ncorrelation with human judgements. Experiments on three tasks, i.e., Shopping\nUtterance Generation, Product Question Generation and Query Auto Completion,\ndemonstrate that our metrics are effective for evaluating STG tasks, and\nimprove the agreement with human judgement up to 20% with respect to common NLG\nmetrics. We believe these findings can pave the way towards better solutions\nfor estimating the representativeness of synthetic text data.\n","authors":["Simone Filice","Jason Ingyu Choi","Giuseppe Castellucci","Eugene Agichtein","Oleg Rokhlenko"],"pdf_url":"https://arxiv.org/pdf/2311.12534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18098v3","updated":"2023-11-21T11:01:24Z","published":"2023-05-29T14:07:52Z","title":"BigTranslate: Augmenting Large Language Models with Multilingual\n  Translation Capability over 100 Languages","summary":"  Large language models (LLMs) demonstrate promising translation performance\namong various natural languages. However, many LLMs especially the open-sourced\nones, such as BLOOM and LLaMA, are English-dominant and support only dozens of\nnatural languages, making the potential of LLMs on language translation less\nexplored. In this work, we present BigTranslate which adapts LLaMA that covers\nonly 20 languages and enhances it with multilingual translation capability on\nmore than 100 languages. BigTranslate is built upon LLaMA-13B and it is\noptimized in three steps. First, we continue training LLaMA with massive\nChinese monolingual data. Second, we continue training the model with a\nlarge-scale parallel dataset that covers 102 natural languages. Third, we\ninstruct-tune the foundation model with multilingual translation instructions,\nleading to our BigTranslate model. The preliminary experiments on multilingual\ntranslation show that BigTranslate performs comparably with ChatGPT and Google\nTranslate in many languages and even outperforms ChatGPT in 8 language pairs.\nWe release the BigTranslate model and hope it can advance the research\nprogress.\n","authors":["Wen Yang","Chong Li","Jiajun Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2305.18098v3.pdf","comment":"16 pages, 4 figures. Our model is available at\n  https://github.com/ZNLP/BigTranslate"},{"id":"http://arxiv.org/abs/2311.12489v1","updated":"2023-11-21T09:59:29Z","published":"2023-11-21T09:59:29Z","title":"Multilingual Word Embeddings for Low-Resource Languages using Anchors\n  and a Chain of Related Languages","summary":"  Very low-resource languages, having only a few million tokens worth of data,\nare not well-supported by multilingual NLP approaches due to poor quality\ncross-lingual word representations. Recent work showed that good cross-lingual\nperformance can be achieved if a source language is related to the low-resource\ntarget language. However, not all language pairs are related. In this paper, we\npropose to build multilingual word embeddings (MWEs) via a novel language\nchain-based approach, that incorporates intermediate related languages to\nbridge the gap between the distant source and target. We build MWEs one\nlanguage at a time by starting from the resource rich source and sequentially\nadding each language in the chain till we reach the target. We extend a\nsemi-joint bilingual approach to multiple languages in order to eliminate the\nmain weakness of previous works, i.e., independently trained monolingual\nembeddings, by anchoring the target language around the multilingual space. We\nevaluate our method on bilingual lexicon induction for 4 language families,\ninvolving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M)\ntarget languages, showing improved performance in both categories.\nAdditionally, our analysis reveals the importance of good quality embeddings\nfor intermediate languages as well as the importance of leveraging anchor\npoints from all languages in the multilingual space.\n","authors":["Viktor Hangya","Silvia Severini","Radoslav Ralev","Alexander Fraser","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2311.12489v1.pdf","comment":"Accepted at the MRL 2023 workshop"},{"id":"http://arxiv.org/abs/2310.17940v3","updated":"2023-11-21T09:47:04Z","published":"2023-10-27T07:34:51Z","title":"Unified Segment-to-Segment Framework for Simultaneous Sequence\n  Generation","summary":"  Simultaneous sequence generation is a pivotal task for real-time scenarios,\nsuch as streaming speech recognition, simultaneous machine translation and\nsimultaneous speech translation, where the target sequence is generated while\nreceiving the source sequence. The crux of achieving high-quality generation\nwith low latency lies in identifying the optimal moments for generating,\naccomplished by learning a mapping between the source and target sequences.\nHowever, existing methods often rely on task-specific heuristics for different\nsequence types, limiting the model's capacity to adaptively learn the\nsource-target mapping and hindering the exploration of multi-task learning for\nvarious simultaneous tasks. In this paper, we propose a unified\nsegment-to-segment framework (Seg2Seg) for simultaneous sequence generation,\nwhich learns the mapping in an adaptive and unified manner. During the process\nof simultaneous generation, the model alternates between waiting for a source\nsegment and generating a target segment, making the segment serve as the\nnatural bridge between the source and target. To accomplish this, Seg2Seg\nintroduces a latent segment as the pivot between source to target and explores\nall potential source-target mappings via the proposed expectation training,\nthereby learning the optimal moments for generating. Experiments on multiple\nsimultaneous generation tasks demonstrate that Seg2Seg achieves\nstate-of-the-art performance and exhibits better generality across various\ntasks.\n","authors":["Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2310.17940v3.pdf","comment":"Grammatical errors prevent the article from being indexed. This is\n  not a problem that can be solved by replacing a new version"},{"id":"http://arxiv.org/abs/2311.12480v1","updated":"2023-11-21T09:44:33Z","published":"2023-11-21T09:44:33Z","title":"Speaker-Adapted End-to-End Visual Speech Recognition for Continuous\n  Spanish","summary":"  Different studies have shown the importance of visual cues throughout the\nspeech perception process. In fact, the development of audiovisual approaches\nhas led to advances in the field of speech technologies. However, although\nnoticeable results have recently been achieved, visual speech recognition\nremains an open research problem. It is a task in which, by dispensing with the\nauditory sense, challenges such as visual ambiguities and the complexity of\nmodeling silence must be faced. Nonetheless, some of these challenges can be\nalleviated when the problem is approached from a speaker-dependent perspective.\nThus, this paper studies, using the Spanish LIP-RTVE database, how the\nestimation of specialized end-to-end systems for a specific person could affect\nthe quality of speech recognition. First, different adaptation strategies based\non the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention\narchitecture was used as a baseline throughout our experiments. Our findings\nshowed that a two-step fine-tuning process, where the VSR system is first\nadapted to the task domain, provided significant improvements when the speaker\nadaptation was addressed. Furthermore, results comparable to the current state\nof the art were reached even when only a limited amount of data was available.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2311.12480v1.pdf","comment":"Accepted in Proceedings of IberSpeech 2022 (\n  https://www.isca-speech.org/archive/iberspeech_2022/gimenogomez22_iberspeech.html\n  )"},{"id":"http://arxiv.org/abs/2311.12475v1","updated":"2023-11-21T09:37:42Z","published":"2023-11-21T09:37:42Z","title":"PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with\n  Unassimilated Loanwords","summary":"  While WangchanBERTa has become the de facto standard in transformer-based\nThai language modeling, it still has shortcomings in regard to the\nunderstanding of foreign words, most notably English words, which are often\nborrowed without orthographic assimilation into Thai in many contexts. We\nidentify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the\nmain source of these shortcomings. We then expand WangchanBERTa's vocabulary\nvia vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new\nmodel using the expanded tokenizer, starting from WangchanBERTa's checkpoint,\non a new dataset that is larger than the one used to train WangchanBERTa. Our\nresults show that our new pretrained model, PhayaThaiBERT, outperforms\nWangchanBERTa in many downstream tasks and datasets.\n","authors":["Panyut Sriwirote","Jalinee Thapiang","Vasan Timtong","Attapol T. Rutherford"],"pdf_url":"https://arxiv.org/pdf/2311.12475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12474v1","updated":"2023-11-21T09:36:11Z","published":"2023-11-21T09:36:11Z","title":"CSMeD: Bridging the Dataset Gap in Automated Citation Screening for\n  Systematic Literature Reviews","summary":"  Systematic literature reviews (SLRs) play an essential role in summarising,\nsynthesising and validating scientific evidence. In recent years, there has\nbeen a growing interest in using machine learning techniques to automate the\nidentification of relevant studies for SLRs. However, the lack of standardised\nevaluation datasets makes comparing the performance of such automated\nliterature screening systems difficult. In this paper, we analyse the citation\nscreening evaluation datasets, revealing that many of the available datasets\nare either too small, suffer from data leakage or have limited applicability to\nsystems treating automated literature screening as a classification task, as\nopposed to, for example, a retrieval or question-answering task. To address\nthese challenges, we introduce CSMeD, a meta-dataset consolidating nine\npublicly released collections, providing unified access to 325 SLRs from the\nfields of medicine and computer science. CSMeD serves as a comprehensive\nresource for training and evaluating the performance of automated citation\nscreening models. Additionally, we introduce CSMeD-FT, a new dataset designed\nexplicitly for evaluating the full text publication screening task. To\ndemonstrate the utility of CSMeD, we conduct experiments and establish\nbaselines on new datasets.\n","authors":["Wojciech Kusa","Oscar E. Mendoza","Matthias Samwald","Petr Knoth","Allan Hanbury"],"pdf_url":"https://arxiv.org/pdf/2311.12474v1.pdf","comment":"Accepted at NeurIPS 2023 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2311.12468v1","updated":"2023-11-21T09:28:00Z","published":"2023-11-21T09:28:00Z","title":"Analysis of Visual Features for Continuous Lipreading in Spanish","summary":"  During a conversation, our brain is responsible for combining information\nobtained from multiple senses in order to improve our ability to understand the\nmessage we are perceiving. Different studies have shown the importance of\npresenting visual information in these situations. Nevertheless, lipreading is\na complex task whose objective is to interpret speech when audio is not\navailable. By dispensing with a sense as crucial as hearing, it will be\nnecessary to be aware of the challenge that this lack presents. In this paper,\nwe propose an analysis of different speech visual features with the intention\nof identifying which of them is the best approach to capture the nature of lip\nmovements for natural Spanish and, in this way, dealing with the automatic\nvisual speech recognition task. In order to estimate our system, we present an\naudiovisual corpus compiled from a subset of the RTVE database, which has been\nused in the Albayz\\'in evaluations. We employ a traditional system based on\nHidden Markov Models with Gaussian Mixture Models. Results show that, although\nthe task is difficult, in restricted conditions we obtain recognition results\nwhich determine that using eigenlips in combination with deep features is the\nbest visual approach.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2311.12468v1.pdf","comment":"Accepted in Proceedings of IberSpeech 2020 (\n  https://www.isca-speech.org/archive/iberspeech_2021/gimenogomez21_iberspeech.html\n  )"},{"id":"http://arxiv.org/abs/2310.18168v3","updated":"2023-11-21T09:19:03Z","published":"2023-10-27T14:27:43Z","title":"Personas as a Way to Model Truthfulness in Language Models","summary":"  Large Language Models (LLMs) are trained on vast amounts of text from the\ninternet, which contains both factual and misleading information about the\nworld. Can language models discern truth from falsehood in this contradicting\ndata? Expanding on the view that LLMs can model different communicative agents,\nwe present the persona hypothesis: LLMs can cluster agents into personas using\ncommon features of their generations. For instance, a truthful persona is a\ngroup of agents that are likely to produce truthful text and that share similar\nfeatures like formal writing styles and scientific references. By modeling this\npersona, LLMs can generalize truthfulness beyond the specific contexts in which\neach agent generated the training text. For example, the model can infer that\nthe agent ``Wikipedia'' will behave truthfully on topics that were only\ngenerated by ``Science'' because they both belong to the truthful persona. We\nshow evidence for the persona hypothesis via two observations: (1) we can probe\nwhether a model's answer will be truthful before it is generated; (2)\nfinetuning a model on a set of facts improves its truthfulness on unseen\ntopics. Next, using arithmetics as a synthetic environment, we show that\nlanguage models can separate true and false statements, and generalize\ntruthfulness across agents; but only if agents in the training data share a\ntruthful generative process that enables the creation of a truthful persona.\nOverall, our findings suggest that models can exploit hierarchical structures\nin the data to learn abstract concepts like truthfulness.\n","authors":["Nitish Joshi","Javier Rando","Abulhair Saparov","Najoung Kim","He He"],"pdf_url":"https://arxiv.org/pdf/2310.18168v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12457v1","updated":"2023-11-21T09:12:21Z","published":"2023-11-21T09:12:21Z","title":"LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild","summary":"  Speech is considered as a multi-modal process where hearing and vision are\ntwo fundamentals pillars. In fact, several studies have demonstrated that the\nrobustness of Automatic Speech Recognition systems can be improved when audio\nand visual cues are combined to represent the nature of speech. In addition,\nVisual Speech Recognition, an open research problem whose purpose is to\ninterpret speech by reading the lips of the speaker, has been a focus of\ninterest in the last decades. Nevertheless, in order to estimate these systems\nin the currently Deep Learning era, large-scale databases are required. On the\nother hand, while most of these databases are dedicated to English, other\nlanguages lack sufficient resources. Thus, this paper presents a\nsemi-automatically annotated audiovisual database to deal with unconstrained\nnatural Spanish, providing 13 hours of data extracted from Spanish television.\nFurthermore, baseline results for both speaker-dependent and\nspeaker-independent scenarios are reported using Hidden Markov Models, a\ntraditional paradigm that has been widely used in the field of Speech\nTechnologies.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2311.12457v1.pdf","comment":"Accepted in Proceedings of LREC 2022 (\n  https://aclanthology.org/2022.lrec-1.294 )"},{"id":"http://arxiv.org/abs/2311.12420v1","updated":"2023-11-21T08:20:39Z","published":"2023-11-21T08:20:39Z","title":"How Far Have We Gone in Vulnerability Detection Using Large Language\n  Models","summary":"  As software becomes increasingly complex and prone to vulnerabilities,\nautomated vulnerability detection is critically important, yet challenging.\nGiven the significant successes of Large Language Models (LLMs) in various\ntasks, there is growing anticipation of their efficacy in vulnerability\ndetection. However, a quantitative understanding of their potential in\nvulnerability detection is still missing. To bridge this gap, we introduce a\ncomprehensive vulnerability benchmark VulBench. This benchmark aggregates\nhigh-quality data from a wide range of CTF (Capture-the-Flag) challenges and\nreal-world applications, with annotations for each vulnerable function\ndetailing the vulnerability type and its root cause. Through our experiments\nencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models\nand static analyzers, we find that several LLMs outperform traditional deep\nlearning approaches in vulnerability detection, revealing an untapped potential\nin LLMs. This work contributes to the understanding and utilization of LLMs for\nenhanced software security.\n","authors":["Zeyu Gao","Hao Wang","Yuchen Zhou","Wenyu Zhu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12418v1","updated":"2023-11-21T08:15:01Z","published":"2023-11-21T08:15:01Z","title":"Visual Analytics for Generative Transformer Models","summary":"  While transformer-based models have achieved state-of-the-art results in a\nvariety of classification and generation tasks, their black-box nature makes\nthem challenging for interpretability. In this work, we present a novel visual\nanalytical framework to support the analysis of transformer-based generative\nnetworks. In contrast to previous work, which has mainly focused on\nencoder-based models, our framework is one of the first dedicated to supporting\nthe analysis of transformer-based encoder-decoder models and decoder-only\nmodels for generative and classification tasks. Hence, we offer an intuitive\noverview that allows the user to explore different facets of the model through\ninteractive visualization. To demonstrate the feasibility and usefulness of our\nframework, we present three detailed case studies based on real-world NLP\nresearch problems.\n","authors":["Raymond Li","Ruixin Yang","Wen Xiao","Ahmed AbuRaed","Gabriel Murray","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2311.12418v1.pdf","comment":"6 pages (reference excluded), 7 figures"},{"id":"http://arxiv.org/abs/2311.12410v1","updated":"2023-11-21T07:56:30Z","published":"2023-11-21T07:56:30Z","title":"nach0: Multimodal Natural and Chemical Languages Foundation Model","summary":"  Large Language Models (LLMs) have substantially driven scientific progress in\nvarious domains, and many papers have demonstrated their ability to tackle\ncomplex problems with creative solutions. Our paper introduces a new foundation\nmodel, nach0, capable of solving various chemical and biological tasks:\nbiomedical question answering, named entity recognition, molecular generation,\nmolecular synthesis, attributes prediction, and others. nach0 is a multi-domain\nand multi-task encoder-decoder LLM pre-trained on unlabeled text from\nscientific literature, patents, and molecule strings to incorporate a range of\nchemical and linguistic knowledge. We employed instruction tuning, where\nspecific task-related instructions are utilized to fine-tune nach0 for the\nfinal set of tasks. To train nach0 effectively, we leverage the NeMo framework,\nenabling efficient parallel optimization of both base and large model versions.\nExtensive experiments demonstrate that our model outperforms state-of-the-art\nbaselines on single-domain and cross-domain tasks. Furthermore, it can generate\nhigh-quality outputs in molecular and textual formats, showcasing its\neffectiveness in multi-domain setups.\n","authors":["Micha Livne","Zulfat Miftahutdinov","Elena Tutubalina","Maksim Kuznetsov","Daniil Polykovskiy","Annika Brundyn","Aastha Jhunjhunwala","Anthony Costa","Alex Aliper","Alex Zhavoronkov"],"pdf_url":"https://arxiv.org/pdf/2311.12410v1.pdf","comment":"Submitted to Nature Communications"},{"id":"http://arxiv.org/abs/2310.07161v2","updated":"2023-11-21T07:54:34Z","published":"2023-10-11T03:19:22Z","title":"Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms","summary":"  Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,\nthe complexities introduced by acoustic transformations merit rigorous\nanalysis. This research, rooted in the exploration of proprietary sender-side\ndenoising effects, meticulously evaluates platforms such as Google Meets and\nZoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,\nensuring a structured examination tailored to various denoising settings and\nreceiver interfaces. A methodological novelty is introduced via the Oaxaca\ndecomposition, traditionally an econometric tool, repurposed herein to analyze\nacoustic-phonetic perturbations within VoIP systems. To further ground the\nimplications of these transformations, psychoacoustic metrics, specifically\nPESQ and STOI, were harnessed to furnish a comprehensive understanding of\nspeech alterations. Cumulatively, the insights garnered underscore the\nintricate landscape of VoIP-influenced acoustic dynamics. In addition to the\nprimary findings, a multitude of metrics are reported, extending the research\npurview. Moreover, out-of-domain benchmarking for both time and time-frequency\ndomain speech enhancement models is included, thereby enhancing the depth and\napplicability of this inquiry. Repository:\ngithub.com/deepology/VoIP-DNS-Challenge\n","authors":["Joseph Konan","Ojas Bhargave","Shikhar Agnihotri","Shuo Han","Yunyang Zeng","Ankit Shah","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2310.07161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12405v1","updated":"2023-11-21T07:50:53Z","published":"2023-11-21T07:50:53Z","title":"IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian\n  Local Languages","summary":"  Significant progress has been made on Indonesian NLP. Nevertheless,\nexploration of the code-mixing phenomenon in Indonesian is limited, despite\nmany languages being frequently mixed with Indonesian in daily conversation. In\nthis work, we explore code-mixing in Indonesian with four embedded languages,\ni.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a\nframework to evaluate and improve the code-mixing robustness. Our analysis\nshows that the pre-training corpus bias affects the model's ability to better\nhandle Indonesian-English code-mixing when compared to other local languages,\ndespite having higher language diversity.\n","authors":["Muhammad Farid Adilazuarda","Samuel Cahyawijaya","Genta Indra Winata","Pascale Fung","Ayu Purwarianti"],"pdf_url":"https://arxiv.org/pdf/2311.12405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12404v1","updated":"2023-11-21T07:43:50Z","published":"2023-11-21T07:43:50Z","title":"InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk\n  Factors in Reddit Posts","summary":"  Mental health professionals and clinicians have observed the upsurge of\nmental disorders due to Interpersonal Risk Factors (IRFs). To simulate the\nhuman-in-the-loop triaging scenario for early detection of mental health\ndisorders, we recognized textual indications to ascertain these IRFs : Thwarted\nBelongingness (TBe) and Perceived Burdensomeness (PBu) within personal\nnarratives. In light of this, we use N-shot learning with GPT-3 model on the\nIRF dataset, and underscored the importance of fine-tuning GPT-3 model to\nincorporate the context-specific sensitivity and the interconnectedness of\ntextual cues that represent both IRFs.\n  In this paper, we introduce an Interpretable Prompting (InterPrompt)} method\nto boost the attention mechanism by fine-tuning the GPT-3 model. This allows a\nmore sophisticated level of language modification by adjusting the pre-trained\nweights. Our model learns to detect usual patterns and underlying connections\nacross both the IRFs, which leads to better system-level explainability and\ntrustworthiness. The results of our research demonstrate that all four variants\nof GPT-3 model, when fine-tuned with InterPrompt, perform considerably better\nas compared to the baseline methods, both in terms of classification and\nexplanation generation.\n","authors":["MSVPJ Sathvik","Surjodeep Sarkar","Chandni Saxena","Sunghwan Sohn","Muskan Garg"],"pdf_url":"https://arxiv.org/pdf/2311.12404v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2311.12399v1","updated":"2023-11-21T07:22:48Z","published":"2023-11-21T07:22:48Z","title":"A Survey of Graph Meets Large Language Model: Progress and Future\n  Directions","summary":"  Graph plays a significant role in representing and analyzing complex\nrelationships in real-world applications such as citation networks, social\nnetworks, and biological data. Recently, Large Language Models (LLMs), which\nhave achieved tremendous success in various domains, have also been leveraged\nin graph-related tasks to surpass traditional Graph Neural Networks (GNNs)\nbased methods and yield state-of-the-art performance. In this survey, we first\npresent a comprehensive review and analysis of existing methods that integrate\nLLMs with graphs. First of all, we propose a new taxonomy, which organizes\nexisting methods into three categories based on the role (i.e., enhancer,\npredictor, and alignment component) played by LLMs in graph-related tasks. Then\nwe systematically survey the representative methods along the three categories\nof the taxonomy. Finally, we discuss the remaining limitations of existing\nstudies and highlight promising avenues for future research. The relevant\npapers are summarized and will be consistently updated at:\nhttps://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.\n","authors":["Yuhan Li","Zhixun Li","Peisong Wang","Jia Li","Xiangguo Sun","Hong Cheng","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2311.12399v1.pdf","comment":"Work in progress; 13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.10777v2","updated":"2023-11-21T07:15:57Z","published":"2023-11-16T06:01:47Z","title":"A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains,\n  Methods, and Trends","summary":"  Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment\nanalysis (SA) that identifies aspects and the associated opinions from a given\ntext. In the digital era, ABSA gained increasing popularity and applications in\nmining opinionated text data to obtain insights and support decisions. ABSA\nresearch employs linguistic, statistical, and machine-learning approaches and\nutilises resources such as labelled datasets, aspect and sentiment lexicons and\nontology. By its nature, ABSA is domain-dependent and can be sensitive to the\nimpact of misalignment between the resource and application domains. However,\nto our knowledge, this topic has not been explored by the existing ABSA\nliterature reviews. In this paper, we present a Systematic Literature Review\n(SLR) of ABSA studies with a focus on the research application domain, dataset\ndomain, and the research methods to examine their relationships and identify\ntrends over time. Our results suggest a number of potential systemic issues in\nthe ABSA research literature, including the predominance of the\n``product/service review'' dataset domain among the majority of studies that\ndid not have a specific research application domain, coupled with the\nprevalence of dataset-reliant methods such as supervised machine learning. This\nreview makes a number of unique contributions to the ABSA research field: 1) To\nour knowledge, it is the first SLR that links the research domain, dataset\ndomain, and research method through a systematic perspective; 2) it is one of\nthe largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191\nsearch results without time constraint; and 3) our review methodology adopted\nan innovative automatic filtering process based on PDF-mining, which enhanced\nscreening quality and reliability. Suggestions and our review limitations are\nalso discussed.\n","authors":["Yan Cathy Hua","Paul Denny","Katerina Taskova","Jörg Wicker"],"pdf_url":"https://arxiv.org/pdf/2311.10777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12395v1","updated":"2023-11-21T07:11:39Z","published":"2023-11-21T07:11:39Z","title":"Problems of Non-equivalent Words in Technical Translation","summary":"  Translating words which do not have equivalent in target language is not easy\nand finding proper equivalent of those words are very important to render\ncorrectly and understandably, the article defines some thoughts and ideas of\nscientists on the common problems of non-equivalent words from English to\nRussian language and includes English and Russian examples and ideas of certain\nscientist. The English language is worldwide spoken and there are 1.35 billion\nEnglish speakers and over 258 million Russian speakers according to the 2021s\nstatistics. Inevitably, these billions of speakers around the world have\nconnection and they may have deal in different criteria. In order to understand\none another they need to have a pure and fully-understood language. These pure\nlanguages understanding directly relates to translation knowledge where\nlinguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. Hence, this research paper includes different ways and rules of\nrendering non-equivalent words from source language to the target language.\n","authors":["Mohammad Ibrahim Qani"],"pdf_url":"https://arxiv.org/pdf/2311.12395v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.10770v2","updated":"2023-11-21T06:59:59Z","published":"2023-11-15T18:42:50Z","title":"Exponentially Faster Language Modelling","summary":"  Language models only really need to use an exponential fraction of their\nneurons for individual inferences. As proof, we present UltraFastBERT, a BERT\nvariant that uses 0.3% of its neurons during inference while performing on par\nwith similar BERT models. UltraFastBERT selectively engages just 12 out of 4095\nneurons for each layer inference. This is achieved by replacing feedforward\nnetworks with fast feedforward networks (FFFs). While no truly efficient\nimplementation currently exists to unlock the full acceleration potential of\nconditional neural execution, we provide high-level CPU code achieving 78x\nspeedup over the optimized baseline feedforward implementation, and a PyTorch\nimplementation delivering 40x speedup over the equivalent batched feedforward\ninference. We publish our training code, benchmarking setup, and model weights.\n","authors":["Peter Belcak","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2311.10770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12375v1","updated":"2023-11-21T06:27:25Z","published":"2023-11-21T06:27:25Z","title":"The Obscure Limitation of Modular Multilingual Language Models","summary":"  We expose the limitation of modular multilingual language models (MLMs) in\nmultilingual inference scenarios with unknown languages. Existing evaluations\nof modular MLMs exclude the involvement of language identification (LID)\nmodules, which obscures the performance of real-case multilingual scenarios of\nmodular MLMs. In this work, we showcase the effect of adding LID on the\nmultilingual evaluation of modular MLMs and provide discussions for closing the\nperformance gap of caused by the pipelined approach of LID and modular MLMs.\n","authors":["Muhammad Farid Adilazuarda","Samuel Cahyawijaya","Ayu Purwarianti"],"pdf_url":"https://arxiv.org/pdf/2311.12375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12373v1","updated":"2023-11-21T06:23:38Z","published":"2023-11-21T06:23:38Z","title":"Beyond Turing: A Comparative Analysis of Approaches for Detecting\n  Machine-Generated Text","summary":"  Significant progress has been made on text generation by pre-trained language\nmodels (PLMs), yet distinguishing between human and machine-generated text\nposes an escalating challenge. This paper offers an in-depth evaluation of\nthree distinct methods used to address this task: traditional shallow learning,\nLanguage Model (LM) fine-tuning, and Multilingual Model fine-tuning. These\napproaches are rigorously tested on a wide range of machine-generated texts,\nproviding a benchmark of their competence in distinguishing between\nhuman-authored and machine-authored linguistic constructs. The results reveal\nconsiderable differences in performance across methods, thus emphasizing the\ncontinued need for advancement in this crucial area of NLP. This study offers\nvaluable insights and paves the way for future research aimed at creating\nrobust and highly discriminative models.\n","authors":["Muhammad Farid Adilazuarda","Nikolaos Nektarios Arkoulis","Oleksii Chumakov"],"pdf_url":"https://arxiv.org/pdf/2311.12373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12355v1","updated":"2023-11-21T05:15:56Z","published":"2023-11-21T05:15:56Z","title":"Utilizing Language Models for Tour Itinerary Recommendation","summary":"  Tour itinerary recommendation involves planning a sequence of relevant\nPoint-of-Interest (POIs), which combines challenges from the fields of both\nOperations Research (OR) and Recommendation Systems (RS). As an OR problem,\nthere is the need to maximize a certain utility (e.g., popularity of POIs in\nthe tour) while adhering to some constraints (e.g., maximum time for the tour).\nAs a RS problem, it is heavily related to problem or filtering or ranking a\nsubset of POIs that are relevant to a user and recommending it as part of an\nitinerary. In this paper, we explore the use of language models for the task of\ntour itinerary recommendation and planning. This task has the unique\nrequirement of recommending personalized POIs relevant to users and planning\nthese POIs as an itinerary that satisfies various constraints. We discuss some\napproaches in this area, such as using word embedding techniques like Word2Vec\nand GloVe for learning POI embeddings and transformer-based techniques like\nBERT for generating\n  itineraries.\n","authors":["Ngai Lam Ho","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2311.12355v1.pdf","comment":"PMAI23 @IJCAI 2023 2nd International Workshop on Process Management\n  in the AI era"},{"id":"http://arxiv.org/abs/2309.17453v2","updated":"2023-11-21T05:04:49Z","published":"2023-09-29T17:59:56Z","title":"Efficient Streaming Language Models with Attention Sinks","summary":"  Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na ``sink'' even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n","authors":["Guangxuan Xiao","Yuandong Tian","Beidi Chen","Song Han","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2309.17453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12351v1","updated":"2023-11-21T04:59:17Z","published":"2023-11-21T04:59:17Z","title":"Advancing Transformer Architecture in Long-Context Large Language\n  Models: A Comprehensive Survey","summary":"  With the bomb ignited by ChatGPT, Transformer-based Large Language Models\n(LLMs) have paved a revolutionary path toward Artificial General Intelligence\n(AGI) and have been applied in diverse areas as knowledge bases, human\ninterfaces, and dynamic agents. However, a prevailing limitation exists: many\ncurrent LLMs, constrained by resources, are primarily pre-trained on shorter\ntexts, rendering them less effective for longer-context prompts, commonly\nencountered in real-world settings. In this paper, we present a comprehensive\nsurvey focusing on the advancement of model architecture in Transformer-based\nLLMs to optimize long-context capabilities across all stages from pre-training\nto inference. We firstly delineate and analyze the problems of handling\nlong-context input and output with the current Transformer-based models. Then,\nwe mainly offer a holistic taxonomy to navigate the landscape of Transformer\nupgrades on architecture to solve these problems. Afterward, we provide the\ninvestigation on wildly used evaluation necessities tailored for long-context\nLLMs, including datasets, metrics, and baseline models, as well as some amazing\noptimization toolkits like libraries, systems, and compilers to augment LLMs'\nefficiency and efficacy across different stages. Finally, we further discuss\nthe predominant challenges and potential avenues for future research in this\ndomain. Additionally, we have established a repository where we curate relevant\nliterature with real-time updates at\nhttps://github.com/Strivin0311/long-llms-learning.\n","authors":["Yunpeng Huang","Jingwei Xu","Zixu Jiang","Junyu Lai","Zenan Li","Yuan Yao","Taolue Chen","Lijuan Yang","Zhou Xin","Xiaoxing Ma"],"pdf_url":"https://arxiv.org/pdf/2311.12351v1.pdf","comment":"35 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2311.12337v1","updated":"2023-11-21T04:06:08Z","published":"2023-11-21T04:06:08Z","title":"Do Smaller Language Models Answer Contextualised Questions Through\n  Memorisation Or Generalisation?","summary":"  A distinction is often drawn between a model's ability to predict a label for\nan evaluation sample that is directly memorised from highly similar training\nsamples versus an ability to predict the label via some method of\ngeneralisation. In the context of using Language Models for question-answering,\ndiscussion continues to occur as to the extent to which questions are answered\nthrough memorisation. We consider this issue for questions that would ideally\nbe answered through reasoning over an associated context. We propose a method\nof identifying evaluation samples for which it is very unlikely our model would\nhave memorised the answers. Our method is based on semantic similarity of input\ntokens and label tokens between training and evaluation samples. We show that\nour method offers advantages upon some prior approaches in that it is able to\nsurface evaluation-train pairs that have overlap in either contiguous or\ndiscontiguous sequences of tokens. We use this method to identify unmemorisable\nsubsets of our evaluation datasets. We train two Language Models in a multitask\nfashion whereby the second model differs from the first only in that it has two\nadditional datasets added to the training regime that are designed to impart\nsimple numerical reasoning strategies of a sort known to improve performance on\nsome of our evaluation datasets but not on others. We then show that there is\nperformance improvement between the two models on the unmemorisable subsets of\nthe evaluation datasets that were expected to benefit from the additional\ntraining datasets. Specifically, performance on unmemorisable subsets of two of\nour evaluation datasets, DROP and ROPES significantly improves by 9.0%, and\n25.7% respectively while other evaluation datasets have no significant change\nin performance.\n","authors":["Tim Hartill","Joshua Bensemann","Michael Witbrock","Patricia J. Riddle"],"pdf_url":"https://arxiv.org/pdf/2311.12337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12323v1","updated":"2023-11-21T03:34:20Z","published":"2023-11-21T03:34:20Z","title":"Modeling Political Orientation of Social Media Posts: An Extended\n  Analysis","summary":"  Developing machine learning models to characterize political polarization on\nonline social media presents significant challenges. These challenges mainly\nstem from various factors such as the lack of annotated data, presence of noise\nin social media datasets, and the sheer volume of data. The common research\npractice typically examines the biased structure of online user communities for\na given topic or qualitatively measuring the impacts of polarized topics on\nsocial media. However, there is limited work focusing on analyzing polarization\nat the ground-level, specifically in the social media posts themselves. Such\nexisting analysis heavily relies on annotated data, which often requires\nlaborious human labeling, offers labels only to specific problems, and lacks\nthe ability to determine the near-future bias state of a social media\nconversations. Understanding the degree of political orientation conveyed in\nsocial media posts is crucial for quantifying the bias of online user\ncommunities and investigating the spread of polarized content. In this work, we\nfirst introduce two heuristic methods that leverage on news media bias and post\ncontent to label social media posts. Next, we compare the efficacy and quality\nof heuristically labeled dataset with a randomly sampled human-annotated\ndataset. Additionally, we demonstrate that current machine learning models can\nexhibit improved performance in predicting political orientation of social\nmedia posts, employing both traditional supervised learning and few-shot\nlearning setups. We conduct experiments using the proposed heuristic methods\nand machine learning approaches to predict the political orientation of posts\ncollected from two social media forums with diverse political ideologies: Gab\nand Twitter.\n","authors":["Sadia Kamal","Brenner Little","Jade Gullic","Trevor Harms","Kristin Olofsson","Arunkumar Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2311.12323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12315v1","updated":"2023-11-21T03:17:14Z","published":"2023-11-21T03:17:14Z","title":"AcademicGPT: Empowering Academic Research","summary":"  Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Yet, many of these advanced\nLLMs are tailored for broad, general-purpose applications. In this technical\nreport, we introduce AcademicGPT, designed specifically to empower academic\nresearch. AcademicGPT is a continual training model derived from LLaMA2-70B.\nOur training corpus mainly consists of academic papers, thesis, content from\nsome academic domain, high-quality Chinese data and others. While it may not be\nextensive in data scale, AcademicGPT marks our initial venture into a\ndomain-specific GPT tailored for research area. We evaluate AcademicGPT on\nseveral established public benchmarks such as MMLU and CEval, as well as on\nsome specialized academic benchmarks like PubMedQA, SCIEval, and our\nnewly-created ComputerScienceQA, to demonstrate its ability from general\nknowledge ability, to Chinese ability, and to academic ability. Building upon\nAcademicGPT's foundation model, we also developed several applications catered\nto the academic area, including General Academic Question Answering,\nAI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract\nGeneration.\n","authors":["Shufa Wei","Xiaolong Xu","Xianbiao Qi","Xi Yin","Jun Xia","Jingyi Ren","Peijun Tang","Yuxiang Zhong","Yihao Chen","Xiaoqin Ren","Yuxin Liang","Liankai Huang","Kai Xie","Weikang Gui","Wei Tan","Shuanglong Sun","Yongquan Hu","Qinxian Liu","Nanjin Li","Chihao Dai","Lihua Wang","Xiaohui Liu","Lei Zhang","Yutao Xie"],"pdf_url":"https://arxiv.org/pdf/2311.12315v1.pdf","comment":"Technical Report. arXiv admin note: text overlap with\n  arXiv:2310.12081, arXiv:2310.10053 by other authors"},{"id":"http://arxiv.org/abs/2304.03898v2","updated":"2023-11-21T02:39:06Z","published":"2023-04-08T03:24:05Z","title":"The Short Text Matching Model Enhanced with Knowledge via Contrastive\n  Learning","summary":"  In recent years, short Text Matching tasks have been widely applied in the\nfields ofadvertising search and recommendation. The difficulty lies in the lack\nof semantic information and word ambiguity caused by the short length of the\ntext. Previous works have introduced complement sentences or knowledge bases to\nprovide additional feature information. However, these methods have not fully\ninteracted between the original sentence and the complement sentence, and have\nnot considered the noise issue that may arise from the introduction of external\nknowledge bases. Therefore, this paper proposes a short Text Matching model\nthat combines contrastive learning and external knowledge. The model uses a\ngenerative model to generate corresponding complement sentences and uses the\ncontrastive learning method to guide the model to obtain more semantically\nmeaningful encoding of the original sentence. In addition, to avoid noise, we\nuse keywords as the main semantics of the original sentence to retrieve\ncorresponding knowledge words in the knowledge base, and construct a knowledge\ngraph. The graph encoding model is used to integrate the knowledge base\ninformation into the model. Our designed model achieves state-of-the-art\nperformance on two publicly available Chinese Text Matching datasets,\ndemonstrating the effectiveness of our model.\n","authors":["Ruiqiang Liu","Mengmeng Cui","Hanjie Mai","Qiang Zhang","Shaohua Xu","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2304.03898v2.pdf","comment":"11 pages,2 figures"},{"id":"http://arxiv.org/abs/2311.12298v1","updated":"2023-11-21T02:35:09Z","published":"2023-11-21T02:35:09Z","title":"Noise in Relation Classification Dataset TACRED: Characterization and\n  Reduction","summary":"  The overarching objective of this paper is two-fold. First, to explore\nmodel-based approaches to characterize the primary cause of the noise. in the\nRE dataset TACRED Second, to identify the potentially noisy instances. Towards\nthe first objective, we analyze predictions and performance of state-of-the-art\n(SOTA) models to identify the root cause of noise in the dataset. Our analysis\nof TACRED shows that the majority of the noise in the dataset originates from\nthe instances labeled as no-relation which are negative examples. For the\nsecond objective, we explore two nearest-neighbor-based strategies to\nautomatically identify potentially noisy examples for elimination and\nreannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is\nbased on the assumption that positive examples are clean. Thus, we have used\nfalse-negative predictions to identify noisy negative examples. Whereas, our\nsecond approach, referred to as Extrinsic Strategy, is based on using a clean\nsubset of the dataset to identify potentially noisy negative examples. Finally,\nwe retrained the SOTA models on the eliminated and reannotated dataset. Our\nempirical results based on two SOTA models trained on TACRED-E following the IS\nshow an average 4% F1-score improvement, whereas reannotation (TACRED-R) does\nnot improve the original results. However, following ES, SOTA models show the\naverage F1-score improvement of 3.8% and 4.4% when trained on respective\neliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We\nfurther extended the ES for cleaning positive examples as well, which resulted\nin an average performance improvement of 5.8% and 5.6% for the eliminated\n(TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively.\n","authors":["Akshay Parekh","Ashish Anand","Amit Awekar"],"pdf_url":"https://arxiv.org/pdf/2311.12298v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2311.10899v2","updated":"2023-11-21T02:16:27Z","published":"2023-11-17T22:44:05Z","title":"Extraction and Summarization of Explicit Video Content using Multi-Modal\n  Deep Learning","summary":"  With the increase in video-sharing platforms across the internet, it is\ndifficult for humans to moderate the data for explicit content. Hence, an\nautomated pipeline to scan through video data for explicit content has become\nthe need of the hour. We propose a novel pipeline that uses multi-modal deep\nlearning to first extract the explicit segments of input videos and then\nsummarize their content using text to determine its age appropriateness and age\nrating. We also evaluate our pipeline's effectiveness in the end using standard\nmetrics.\n","authors":["Shaunak Joshi","Raghav Gaggar"],"pdf_url":"https://arxiv.org/pdf/2311.10899v2.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.12289v1","updated":"2023-11-21T02:02:46Z","published":"2023-11-21T02:02:46Z","title":"ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for\n  Interdisciplinary Science","summary":"  Large language models record impressive performance on many natural language\nprocessing tasks. However, their knowledge capacity is limited to the\npretraining corpus. Retrieval augmentation offers an effective solution by\nretrieving context from external knowledge sources to complement the language\nmodel. However, existing retrieval augmentation techniques ignore the\nstructural relationships between these documents. Furthermore, retrieval models\nare not explored much in scientific tasks, especially in regard to the\nfaithfulness of retrieved documents. In this paper, we propose a novel\nstructure-aware retrieval augmented language model that accommodates document\nstructure during retrieval augmentation. We create a heterogeneous document\ngraph capturing multiple types of relationships (e.g., citation, co-authorship,\netc.) that connect documents from more than 15 scientific disciplines (e.g.,\nPhysics, Medicine, Chemistry, etc.). We train a graph neural network on the\ncurated document graph to act as a structural encoder for the corresponding\npassages retrieved during the model pretraining. Particularly, along with text\nembeddings of the retrieved passages, we obtain structural embeddings of the\ndocuments (passages) and fuse them together before feeding them to the language\nmodel. We evaluate our model extensively on various scientific benchmarks that\ninclude science question-answering and scientific document classification\ntasks. Experimental results demonstrate that structure-aware retrieval improves\nretrieving more coherent, faithful and contextually relevant passages, while\nshowing a comparable performance in the overall accuracy.\n","authors":["Sai Munikoti","Anurag Acharya","Sridevi Wagle","Sameera Horawalavithana"],"pdf_url":"https://arxiv.org/pdf/2311.12289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10429v4","updated":"2023-11-21T02:01:53Z","published":"2023-05-17T17:58:13Z","title":"DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining","summary":"  The mixture proportions of pretraining data domains (e.g., Wikipedia, books,\nweb text) greatly affect language model (LM) performance. In this paper, we\npropose Domain Reweighting with Minimax Optimization (DoReMi), which first\ntrains a small proxy model using group distributionally robust optimization\n(Group DRO) over domains to produce domain weights (mixture proportions)\nwithout knowledge of downstream tasks. We then resample a dataset with these\ndomain weights and train a larger, full-sized model. In our experiments, we use\nDoReMi on a 280M-parameter proxy model to set the domain weights for training\nan 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi\nimproves perplexity across all domains, even when it downweights a domain.\nDoReMi improves average few-shot downstream accuracy by 6.5% points over a\nbaseline model trained using The Pile's default domain weights and reaches the\nbaseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi,\nwhich has no knowledge of downstream tasks, even matches the performance of\nusing domain weights tuned on downstream tasks.\n","authors":["Sang Michael Xie","Hieu Pham","Xuanyi Dong","Nan Du","Hanxiao Liu","Yifeng Lu","Percy Liang","Quoc V. Le","Tengyu Ma","Adams Wei Yu"],"pdf_url":"https://arxiv.org/pdf/2305.10429v4.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.12275v1","updated":"2023-11-21T01:34:02Z","published":"2023-11-21T01:34:02Z","title":"Enabling On-Device Large Language Model Personalization with\n  Self-Supervised Data Selection and Synthesis","summary":"  After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.\n","authors":["Ruiyang Qin","Jun Xia","Zhenge Jia","Meng Jiang","Ahmed Abbasi","Peipei Zhou","Jingtong Hu","Yiyu Shi"],"pdf_url":"https://arxiv.org/pdf/2311.12275v1.pdf","comment":"6 pages, 3 figures, 3 tables"}]},"2023-11-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.12233v1","updated":"2023-11-20T23:17:20Z","published":"2023-11-20T23:17:20Z","title":"Unifying Corroborative and Contributive Attributions in Large Language\n  Models","summary":"  As businesses, products, and services spring up around large language models,\nthe trustworthiness of these models hinges on the verifiability of their\noutputs. However, methods for explaining language model outputs largely fall\nacross two distinct fields of study which both use the term \"attribution\" to\nrefer to entirely separate techniques: citation generation and training data\nattribution. In many modern applications, such as legal document generation and\nmedical question answering, both types of attributions are important. In this\nwork, we argue for and present a unified framework of large language model\nattributions. We show how existing methods of different types of attribution\nfall under the unified framework. We also use the framework to discuss\nreal-world use cases where one or both types of attributions are required. We\nbelieve that this unified framework will guide the use case driven development\nof systems that leverage both types of attribution, as well as the\nstandardization of their evaluation.\n","authors":["Theodora Worledge","Judy Hanwen Shen","Nicole Meister","Caleb Winston","Carlos Guestrin"],"pdf_url":"https://arxiv.org/pdf/2311.12233v1.pdf","comment":"NeurIPS ATTRIB Workshop 2023"},{"id":"http://arxiv.org/abs/2307.03172v3","updated":"2023-11-20T23:09:34Z","published":"2023-07-06T17:54:11Z","title":"Lost in the Middle: How Language Models Use Long Contexts","summary":"  While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nthe performance of language models on two tasks that require identifying\nrelevant information in their input contexts: multi-document question answering\nand key-value retrieval. We find that performance can degrade significantly\nwhen changing the position of relevant information, indicating that current\nlanguage models do not robustly make use of information in long input contexts.\nIn particular, we observe that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts, even for explicitly long-context models. Our analysis\nprovides a better understanding of how language models use their input context\nand provides new evaluation protocols for future long-context language models.\n","authors":["Nelson F. Liu","Kevin Lin","John Hewitt","Ashwin Paranjape","Michele Bevilacqua","Fabio Petroni","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2307.03172v3.pdf","comment":"18 pages, 16 figures. Accepted for publication in Transactions of the\n  Association for Computational Linguistics (TACL), 2023"},{"id":"http://arxiv.org/abs/2311.12179v1","updated":"2023-11-20T20:48:25Z","published":"2023-11-20T20:48:25Z","title":"Leveraging Closed-Access Multilingual Embedding for Automatic Sentence\n  Alignment in Low Resource Languages","summary":"  The importance of qualitative parallel data in machine translation has long\nbeen determined but it has always been very difficult to obtain such in\nsufficient quantity for the majority of world languages, mainly because of the\nassociated cost and also the lack of accessibility to these languages. Despite\nthe potential for obtaining parallel datasets from online articles using\nautomatic approaches, forensic investigations have found a lot of\nquality-related issues such as misalignment, and wrong language codes. In this\nwork, we present a simple but qualitative parallel sentence aligner that\ncarefully leveraged the closed-access Cohere multilingual embedding, a solution\nthat ranked second in the just concluded #CoHereAIHack 2023 Challenge (see\nhttps://ai6lagos.devpost.com). The proposed approach achieved $94.96$ and\n$54.83$ f1 scores on FLORES and MAFAND-MT, compared to $3.64$ and $0.64$ of\nLASER respectively. Our method also achieved an improvement of more than 5 BLEU\nscores over LASER, when the resulting datasets were used with MAFAND-MT dataset\nto train translation models. Our code and data are available for research\npurposes here (https://github.com/abumafrim/Cohere-Align).\n","authors":["Idris Abdulmumin","Auwal Abubakar Khalid","Shamsuddeen Hassan Muhammad","Ibrahim Said Ahmad","Lukman Jibril Aliyu","Babangida Sani","Bala Mairiga Abduljalil","Sani Ahmad Hassan"],"pdf_url":"https://arxiv.org/pdf/2311.12179v1.pdf","comment":"To appear in the proceedings of ICCAIT 2023. 6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.06233v3","updated":"2023-11-20T20:43:10Z","published":"2023-11-10T18:48:58Z","title":"Data Contamination Quiz: A Tool to Detect and Estimate Contamination in\n  Large Language Models","summary":"  We propose the Data Contamination Quiz, a simple and effective approach to\ndetect data contamination in large language models (LLMs) and estimate the\namount of it. Specifically, we frame data contamination detection as a series\nof multiple-choice questions. We devise a quiz format wherein three perturbed\nversions of each dataset instance are created. These changes only include\nword-level perturbations, replacing words with their contextual synonyms,\nensuring both the semantic and sentence structure remain exactly the same as\nthe original instance. Together with the original instance, these perturbed\nversions constitute the choices in the quiz. Given that the only distinguishing\nsignal among these choices is the exact wording, an LLM, when tasked with\nidentifying the original instance from the choices, opts for the original if it\nhas memorized it in its pre-training phase--a trait intrinsic to LLMs. A\ndataset partition is then marked as contaminated if the LLM's performance on\nthe quiz surpasses what random chance suggests. Our evaluation spans seven\ndatasets and their respective splits (train and test/validation) on two\nstate-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the\npre-training data, our results suggest that our approach not only enhances the\ndetection of data contamination but also provides an accurate estimation of its\nextent, even when the contamination signal is weak.\n","authors":["Shahriar Golchin","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2311.06233v3.pdf","comment":"v1.2 preprint"},{"id":"http://arxiv.org/abs/2301.02748v4","updated":"2023-11-20T19:45:49Z","published":"2023-01-06T23:34:52Z","title":"Generative Antibody Design for Complementary Chain Pairing Sequences\n  through Encoder-Decoder Language Model","summary":"  Current protein language models (pLMs) predominantly focus on single-chain\nprotein sequences and often have not accounted for constraints on generative\ndesign imposed by protein-protein interactions. To address this gap, we present\npaired Antibody T5 (pAbT5), an encoder-decoder model to generate complementary\nheavy or light chain from its pairing partner. We show that our model respects\nconservation in framework regions and variability in hypervariable domains,\ndemonstrated by agreement with sequence alignment and variable-length CDR\nloops. We also show that our model captures chain pairing preferences through\nthe recovery of ground-truth chain type and gene families. Our results showcase\nthe potential of pAbT5 in generative antibody design, incorporating biological\nconstraints from chain pairing preferences.\n","authors":["Simon K. S. Chu","Kathy Y. Wei"],"pdf_url":"https://arxiv.org/pdf/2301.02748v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12131v1","updated":"2023-11-20T19:28:52Z","published":"2023-11-20T19:28:52Z","title":"Human Learning by Model Feedback: The Dynamics of Iterative Prompting\n  with Midjourney","summary":"  Generating images with a Text-to-Image model often requires multiple trials,\nwhere human users iteratively update their prompt based on feedback, namely the\noutput image. Taking inspiration from cognitive work on reference games and\ndialogue alignment, this paper analyzes the dynamics of the user prompts along\nsuch iterations. We compile a dataset of iterative interactions of human users\nwith Midjourney. Our analysis then reveals that prompts predictably converge\ntoward specific traits along these iterations. We further study whether this\nconvergence is due to human users, realizing they missed important details, or\ndue to adaptation to the model's ``preferences'', producing better images for a\nspecific language style. We show initial evidence that both possibilities are\nat play. The possibility that users adapt to the model's preference raises\nconcerns about reusing user data for further training. The prompts may be\nbiased towards the preferences of a specific model, rather than align with\nhuman intentions and natural manner of expression.\n","authors":["Shachar Don-Yehiya","Leshem Choshen","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2311.12131v1.pdf","comment":"EMNLP23"},{"id":"http://arxiv.org/abs/2311.12023v1","updated":"2023-11-20T18:57:41Z","published":"2023-11-20T18:57:41Z","title":"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient\n  Language Model Finetuning","summary":"  We propose a simple approach for memory-efficient adaptation of pretrained\nlanguage models. Our approach uses an iterative algorithm to decompose each\npretrained matrix into a high-precision low-rank component and a\nmemory-efficient quantized component. During finetuning, the quantized\ncomponent remains fixed and only the low-rank component is updated. We present\nan integer linear programming formulation of the quantization component which\nenables dynamic configuration of quantization parameters (e.g., bit-width,\nblock size) for each matrix given an overall target memory budget. We further\nexplore a data-aware version of the algorithm which uses an approximation of\nthe Fisher information matrix to weight the reconstruction objective during\nmatrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B)\ndemonstrate that our low-rank plus quantized matrix decomposition approach\n(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables\nmore aggressive quantization. For example, on the OpenAssistant benchmark\nLQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a\nmodel finetuned with 4-bit QLoRA. When finetuned on a language modeling\ncalibration dataset, LQ-LoRA can also be used for model compression; in this\nsetting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when\nincluding the low-rank components and requires 27GB of GPU memory) is\ncompetitive with the original model in full precision.\n","authors":["Han Guo","Philip Greengard","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12022v1","updated":"2023-11-20T18:57:34Z","published":"2023-11-20T18:57:34Z","title":"GPQA: A Graduate-Level Google-Proof Q&A Benchmark","summary":"  We present GPQA, a challenging dataset of 448 multiple-choice questions\nwritten by domain experts in biology, physics, and chemistry. We ensure that\nthe questions are high-quality and extremely difficult: experts who have or are\npursuing PhDs in the corresponding domains reach 65% accuracy (74% when\ndiscounting clear mistakes the experts identified in retrospect), while highly\nskilled non-expert validators only reach 34% accuracy, despite spending on\naverage over 30 minutes with unrestricted access to the web (i.e., the\nquestions are \"Google-proof\"). The questions are also difficult for\nstate-of-the-art AI systems, with our strongest GPT-4 based baseline achieving\n39% accuracy. If we are to use future AI systems to help us answer very hard\nquestions, for example, when developing new scientific knowledge, we need to\ndevelop scalable oversight methods that enable humans to supervise their\noutputs, which may be difficult even if the supervisors are themselves skilled\nand knowledgeable. The difficulty of GPQA both for skilled non-experts and\nfrontier AI systems should enable realistic scalable oversight experiments,\nwhich we hope can help devise ways for human experts to reliably get truthful\ninformation from AI systems that surpass human capabilities.\n","authors":["David Rein","Betty Li Hou","Asa Cooper Stickland","Jackson Petty","Richard Yuanzhe Pang","Julien Dirani","Julian Michael","Samuel R. Bowman"],"pdf_url":"https://arxiv.org/pdf/2311.12022v1.pdf","comment":"28 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2311.12015v1","updated":"2023-11-20T18:54:39Z","published":"2023-11-20T18:54:39Z","title":"GPT-4V(ision) for Robotics: Multimodal Task Planning from Human\n  Demonstration","summary":"  We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/\n","authors":["Naoki Wake","Atsushi Kanehira","Kazuhiro Sasabuchi","Jun Takamatsu","Katsushi Ikeuchi"],"pdf_url":"https://arxiv.org/pdf/2311.12015v1.pdf","comment":"8 pages, 10 figures, 1 table. Last updated on November 20th, 2023"},{"id":"http://arxiv.org/abs/2310.15127v2","updated":"2023-11-20T18:51:29Z","published":"2023-10-23T17:31:55Z","title":"Open-Ended Instructable Embodied Agents with Memory-Augmented Large\n  Language Models","summary":"  Pre-trained and frozen large language models (LLMs) can effectively map\nsimple scene rearrangement instructions to programs over a robot's visuomotor\nfunctions through appropriate few-shot example prompting. To parse open-domain\nnatural language and adapt to a user's idiosyncratic procedures, not known\nduring prompt engineering time, fixed prompts fall short. In this paper, we\nintroduce HELPER, an embodied agent equipped with an external memory of\nlanguage-program pairs that parses free-form human-robot dialogue into action\nprograms through retrieval-augmented LLM prompting: relevant memories are\nretrieved based on the current dialogue, instruction, correction, or VLM\ndescription, and used as in-context prompt examples for LLM querying. The\nmemory is expanded during deployment to include pairs of user's language and\naction plans, to assist future inferences and personalize them to the user's\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\nTfD. Our models, code, and video results can be found in our project's website:\nhttps://helper-agent-llm.github.io.\n","authors":["Gabriel Sarch","Yue Wu","Michael J. Tarr","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2310.15127v2.pdf","comment":"Project page with code & videos: https://helper-agent-llm.github.io"},{"id":"http://arxiv.org/abs/2311.11981v1","updated":"2023-11-20T18:16:27Z","published":"2023-11-20T18:16:27Z","title":"H-COAL: Human Correction of AI-Generated Labels for Biomedical Named\n  Entity Recognition","summary":"  With the rapid advancement of machine learning models for NLP tasks,\ncollecting high-fidelity labels from AI models is a realistic possibility.\nFirms now make AI available to customers via predictions as a service (PaaS).\nThis includes PaaS products for healthcare. It is unclear whether these labels\ncan be used for training a local model without expensive annotation checking by\nin-house experts. In this work, we propose a new framework for Human Correction\nof AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one can\nselectively correct labels and approach gold standard performance (100% human\nlabeling) with significantly less human effort. We show that correcting 5% of\nlabels can close the AI-human performance gap by up to 64% relative\nimprovement, and correcting 20% of labels can close the performance gap by up\nto 86% relative improvement.\n","authors":["Xiaojing Duan","John P. Lalor"],"pdf_url":"https://arxiv.org/pdf/2311.11981v1.pdf","comment":"Presented at Conference on Information Systems and Technology (CIST)\n  2023"},{"id":"http://arxiv.org/abs/2311.11979v1","updated":"2023-11-20T18:12:28Z","published":"2023-11-20T18:12:28Z","title":"On the Potential and Limitations of Few-Shot In-Context Learning to\n  Generate Metamorphic Specifications for Tax Preparation Software","summary":"  Due to the ever-increasing complexity of income tax laws in the United\nStates, the number of US taxpayers filing their taxes using tax preparation\nsoftware (henceforth, tax software) continues to increase. According to the\nU.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed\ntheir individual income taxes using tax software. Given the legal consequences\nof incorrectly filing taxes for the taxpayer, ensuring the correctness of tax\nsoftware is of paramount importance. Metamorphic testing has emerged as a\nleading solution to test and debug legal-critical tax software due to the\nabsence of correctness requirements and trustworthy datasets. The key idea\nbehind metamorphic testing is to express the properties of a system in terms of\nthe relationship between one input and its slightly metamorphosed twinned\ninput. Extracting metamorphic properties from IRS tax publications is a tedious\nand time-consuming process. As a response, this paper formulates the task of\ngenerating metamorphic specifications as a translation task between properties\nextracted from tax documents - expressed in natural language - to a contrastive\nfirst-order logic form. We perform a systematic analysis on the potential and\nlimitations of in-context learning with Large Language Models(LLMs) for this\ntask, and outline a research agenda towards automating the generation of\nmetamorphic specifications for tax preparation software.\n","authors":["Dananjay Srinivas","Rohan Das","Saeid Tizpaz-Niari","Ashutosh Trivedi","Maria Leonor Pacheco"],"pdf_url":"https://arxiv.org/pdf/2311.11979v1.pdf","comment":"Accepted to the Proceedings of the Natural Legal Language Processing\n  Workshop, EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.11976v1","updated":"2023-11-20T18:06:03Z","published":"2023-11-20T18:06:03Z","title":"Context-aware Neural Machine Translation for English-Japanese Business\n  Scene Dialogues","summary":"  Despite the remarkable advancements in machine translation, the current\nsentence-level paradigm faces challenges when dealing with highly-contextual\nlanguages like Japanese. In this paper, we explore how context-awareness can\nimprove the performance of the current Neural Machine Translation (NMT) models\nfor English-Japanese business dialogues translation, and what kind of context\nprovides meaningful information to improve translation. As business dialogue\ninvolves complex discourse phenomena but offers scarce training resources, we\nadapted a pretrained mBART model, finetuning on multi-sentence dialogue data,\nwhich allows us to experiment with different contexts. We investigate the\nimpact of larger context sizes and propose novel context tokens encoding\nextra-sentential information, such as speaker turn and scene type. We make use\nof Conditional Cross-Mutual Information (CXMI) to explore how much of the\ncontext the model uses and generalise CXMI to study the impact of the\nextra-sentential context. Overall, we find that models leverage both preceding\nsentences and extra-sentential context (with CXMI increasing with context size)\nand we provide a more focused analysis on honorifics translation. Regarding\ntranslation quality, increased source-side context paired with scene and\nspeaker information improves the model performance compared to previous work\nand our context-agnostic baselines, measured in BLEU and COMET metrics.\n","authors":["Sumire Honda","Patrick Fernandes","Chrysoula Zerva"],"pdf_url":"https://arxiv.org/pdf/2311.11976v1.pdf","comment":"MT Summit 2023, research track, link to paper in proceedings:\n  https://aclanthology.org/2023.mtsummit-research.23/"},{"id":"http://arxiv.org/abs/2311.11973v1","updated":"2023-11-20T18:01:29Z","published":"2023-11-20T18:01:29Z","title":"Adaptive Training Distributions with Scalable Online Bilevel\n  Optimization","summary":"  Large neural networks pretrained on web-scale corpora are central to modern\nmachine learning. In this paradigm, the distribution of the large,\nheterogeneous pretraining data rarely matches that of the application domain.\nThis work considers modifying the pretraining distribution in the case where\none has a small sample of data reflecting the targeted test conditions. We\npropose an algorithm motivated by a recent formulation of this setting as an\nonline, bilevel optimization problem. With scalability in mind, our algorithm\nprioritizes computing gradients at training points which are likely to most\nimprove the loss on the targeted distribution. Empirically, we show that in\nsome cases this approach is beneficial over existing strategies from the domain\nadaptation literature but may not succeed in other cases. We propose a simple\ntest to evaluate when our approach can be expected to work well and point\ntowards further research to address current limitations.\n","authors":["David Grangier","Pierre Ablin","Awni Hannun"],"pdf_url":"https://arxiv.org/pdf/2311.11973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11967v1","updated":"2023-11-20T17:47:37Z","published":"2023-11-20T17:47:37Z","title":"Automatic Analysis of Substantiation in Scientific Peer Reviews","summary":"  With the increasing amount of problematic peer reviews in top AI conferences,\nthe community is urgently in need of automatic quality control measures. In\nthis paper, we restrict our attention to substantiation -- one popular quality\naspect indicating whether the claims in a review are sufficiently supported by\nevidence -- and provide a solution automatizing this evaluation process. To\nachieve this goal, we first formulate the problem as claim-evidence pair\nextraction in scientific peer reviews, and collect SubstanReview, the first\nannotated dataset for this task. SubstanReview consists of 550 reviews from NLP\nconferences annotated by domain experts. On the basis of this dataset, we train\nan argument mining system to automatically analyze the level of substantiation\nin peer reviews. We also perform data analysis on the SubstanReview dataset to\nobtain meaningful insights on peer reviewing quality in NLP conferences over\nrecent years.\n","authors":["Yanzhu Guo","Guokan Shang","Virgile Rennard","Michalis Vazirgiannis","Chloé Clavel"],"pdf_url":"https://arxiv.org/pdf/2311.11967v1.pdf","comment":"Accepted to EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2311.11944v1","updated":"2023-11-20T17:28:02Z","published":"2023-11-20T17:28:02Z","title":"FinanceBench: A New Benchmark for Financial Question Answering","summary":"  FinanceBench is a first-of-its-kind test suite for evaluating the performance\nof LLMs on open book financial question answering (QA). It comprises 10,231\nquestions about publicly traded companies, with corresponding answers and\nevidence strings. The questions in FinanceBench are ecologically valid and\ncover a diverse set of scenarios. They are intended to be clear-cut and\nstraightforward to answer to serve as a minimum performance standard. We test\n16 state of the art model configurations (including GPT-4-Turbo, Llama2 and\nClaude2, with vector stores and long context prompts) on a sample of 150 cases\nfrom FinanceBench, and manually review their answers (n=2,400). The cases are\navailable open-source. We show that existing LLMs have clear limitations for\nfinancial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly\nanswered or refused to answer 81% of questions. While augmentation techniques\nsuch as using longer context window to feed in relevant evidence improve\nperformance, they are unrealistic for enterprise settings due to increased\nlatency and cannot support larger financial documents. We find that all models\nexamined exhibit weaknesses, such as hallucinations, that limit their\nsuitability for use by enterprises.\n","authors":["Pranab Islam","Anand Kannappan","Douwe Kiela","Rebecca Qian","Nino Scherrer","Bertie Vidgen"],"pdf_url":"https://arxiv.org/pdf/2311.11944v1.pdf","comment":"Dataset is available at:\n  https://huggingface.co/datasets/PatronusAI/financebench"},{"id":"http://arxiv.org/abs/2311.10217v2","updated":"2023-11-20T17:08:11Z","published":"2023-11-16T22:15:15Z","title":"A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal\n  Structures","summary":"  The present paper introduces a novel object of study - a language fractal\nstructure. We hypothesize that a set of embeddings of all $n$-grams of a\nnatural language constitutes a representative sample of this fractal set. (We\nuse the term Hailonakea to refer to the sum total of all language fractal\nstructures, over all $n$). The paper estimates intrinsic (genuine) dimensions\nof language fractal structures for the Russian and English languages. To this\nend, we employ methods based on (1) topological data analysis and (2) a minimum\nspanning tree of a data graph for a cloud of points considered (Steele\ntheorem). For both languages, for all $n$, the intrinsic dimensions appear to\nbe non-integer values (typical for fractal sets), close to 9 for both of the\nRussian and English language.\n","authors":["Vasilii A. Gromov","Nikita S. Borodin","Asel S. Yerbolova"],"pdf_url":"https://arxiv.org/pdf/2311.10217v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2209.09757v2","updated":"2023-11-20T16:54:55Z","published":"2022-09-20T14:39:12Z","title":"Language Varieties of Italy: Technology Challenges and Opportunities","summary":"  Italy is characterized by a one-of-a-kind linguistic diversity landscape in\nEurope, which implicitly encodes local knowledge, cultural traditions, artistic\nexpressions and history of its speakers. However, most local languages and\ndialects in Italy are at risk of disappearing within few generations. The NLP\ncommunity has recently begun to engage with endangered languages, including\nthose of Italy. Yet, most efforts assume that these varieties are\nunder-resourced language monoliths with an established written form and\nhomogeneous functions and needs, and thus highly interchangeable with each\nother and with high-resource, standardized languages. In this paper, we\nintroduce the linguistic context of Italy and challenge the default\nmachine-centric assumptions of NLP for Italy's language varieties. We advocate\nfor a shift in the paradigm from machine-centric to speaker-centric NLP, and\nprovide recommendations and opportunities for work that prioritizes languages\nand their speakers over technological advances. To facilitate the process, we\nfinally propose building a local community towards responsible, participatory\nefforts aimed at supporting vitality of languages and dialects of Italy.\n","authors":["Alan Ramponi"],"pdf_url":"https://arxiv.org/pdf/2209.09757v2.pdf","comment":"Accepted to TACL. This arXiv version is a pre-MIT Press publication\n  version"},{"id":"http://arxiv.org/abs/2311.11904v1","updated":"2023-11-20T16:37:45Z","published":"2023-11-20T16:37:45Z","title":"LLMs as Visual Explainers: Advancing Image Classification with Evolving\n  Visual Descriptions","summary":"  Vision-language models (VLMs) offer a promising paradigm for image\nclassification by comparing the similarity between images and class embeddings.\nA critical challenge lies in crafting precise textual representations for class\nnames. While previous studies have leveraged recent advancements in large\nlanguage models (LLMs) to enhance these descriptors, their outputs often suffer\nfrom ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent\nreliance on textual interactions with LLMs, leading to a mismatch between the\ngenerated text and the visual content in VLMs' latent space - a phenomenon we\nterm the \"explain without seeing\" dilemma. 2) The oversight of the inter-class\nrelationships, resulting in descriptors that fail to differentiate similar\nclasses effectively. To address these issues, we propose a novel image\nclassification framework combining VLMs with LLMs, named Iterative Optimization\nwith Visual Feedback. In particular, our method develops an LLM-based agent,\nemploying an evolutionary optimization strategy to refine class descriptors.\nCrucially, we incorporate visual feedback from VLM classification metrics,\nthereby guiding the optimization process with concrete visual data. Our method\nleads to improving accuracy on a wide range of image classification benchmarks,\nwith 3.47\\% average gains over state-of-the-art methods. We also highlight the\nresulting descriptions serve as explainable and robust features that can\nconsistently improve the performance across various backbone models.\n","authors":["Songhao Han","Le Zhuo","Yue Liao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2311.11904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.07496v3","updated":"2023-11-20T15:57:43Z","published":"2022-09-15T17:37:08Z","title":"Unsupervised Opinion Summarization Using Approximate Geodesics","summary":"  Opinion summarization is the task of creating summaries capturing popular\nopinions from user reviews. In this paper, we introduce Geodesic Summarizer\n(GeoSumm), a novel system to perform unsupervised extractive opinion\nsummarization. GeoSumm involves an encoder-decoder based representation\nlearning model, that generates representations of text as a distribution over\nlatent semantic units. GeoSumm generates these representations by performing\ndictionary learning over pre-trained text representations at multiple decoder\nlayers. We then use these representations to quantify the relevance of review\nsentences using a novel approximate geodesic distance based scoring mechanism.\nWe use the relevance scores to identify popular opinions in order to compose\ngeneral and aspect-specific summaries. Our proposed model, GeoSumm, achieves\nstate-of-the-art performance on three opinion summarization datasets. We\nperform additional experiments to analyze the functioning of our model and\nshowcase the generalization ability of {\\X} across different domains.\n","authors":["Somnath Basu Roy Chowdhury","Nicholas Monath","Avinava Dubey","Amr Ahmed","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2209.07496v3.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.11861v1","updated":"2023-11-20T15:57:04Z","published":"2023-11-20T15:57:04Z","title":"Generating Valid and Natural Adversarial Examples with Large Language\n  Models","summary":"  Deep learning-based natural language processing (NLP) models, particularly\npre-trained language models (PLMs), have been revealed to be vulnerable to\nadversarial attacks. However, the adversarial examples generated by many\nmainstream word-level adversarial attack models are neither valid nor natural,\nleading to the loss of semantic maintenance, grammaticality, and human\nimperceptibility. Based on the exceptional capacity of language understanding\nand generation of large language models (LLMs), we propose LLM-Attack, which\naims at generating both valid and natural adversarial examples with LLMs. The\nmethod consists of two stages: word importance ranking (which searches for the\nmost vulnerable words) and word synonym replacement (which substitutes them\nwith their synonyms obtained from LLMs). Experimental results on the Movie\nReview (MR), IMDB, and Yelp Review Polarity datasets against the baseline\nadversarial attack models illustrate the effectiveness of LLM-Attack, and it\noutperforms the baselines in human and GPT-4 evaluation by a significant\nmargin. The model can generate adversarial examples that are typically valid\nand natural, with the preservation of semantic meaning, grammaticality, and\nhuman imperceptibility.\n","authors":["Zimu Wang","Wei Wang","Qi Chen","Qiufeng Wang","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.11861v1.pdf","comment":"Submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2311.11855v1","updated":"2023-11-20T15:50:09Z","published":"2023-11-20T15:50:09Z","title":"Evil Geniuses: Delving into the Safety of LLM-based Agents","summary":"  The rapid advancements in large language models (LLMs) have led to a\nresurgence in LLM-based agents, which demonstrate impressive human-like\nbehaviors and cooperative capabilities in various interactions and strategy\nformulations. However, evaluating the safety of LLM-based agents remains a\ncomplex challenge. This paper elaborately conducts a series of manual jailbreak\nprompts along with a virtual chat-powered evil plan development team, dubbed\nEvil Geniuses, to thoroughly probe the safety aspects of these agents. Our\ninvestigation reveals three notable phenomena: 1) LLM-based agents exhibit\nreduced robustness against malicious attacks. 2) the attacked agents could\nprovide more nuanced responses. 3) the detection of the produced improper\nresponses is more challenging. These insights prompt us to question the\neffectiveness of LLM-based attacks on agents, highlighting vulnerabilities at\nvarious levels and within different role specializations within the\nsystem/agent of LLM-based agents. Extensive evaluation and discussion reveal\nthat LLM-based agents face significant challenges in safety and yield insights\nfor future research. Our code is available at\nhttps://github.com/T1aNS1R/Evil-Geniuses.\n","authors":["Yu Tian","Xiao Yang","Jingyuan Zhang","Yinpeng Dong","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2311.11855v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2311.11846v1","updated":"2023-11-20T15:37:33Z","published":"2023-11-20T15:37:33Z","title":"Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for\n  Parsing Multinational Street Addresses","summary":"  Segmenting an address into meaningful components, also known as address\nparsing, is an essential step in many applications from record linkage to\ngeocoding and package delivery. Consequently, a lot of work has been dedicated\nto develop accurate address parsing techniques, with machine learning and\nneural network methods leading the state-of-the-art scoreboard. However, most\nof the work on address parsing has been confined to academic endeavours with\nlittle availability of free and easy-to-use open-source solutions.\n  This paper presents Deepparse, a Python open-source, extendable, fine-tunable\naddress parsing solution under LGPL-3.0 licence to parse multinational\naddresses using state-of-the-art deep learning algorithms and evaluated on over\n60 countries. It can parse addresses written in any language and use any\naddress standard. The pre-trained model achieves average $99~\\%$ parsing\naccuracies on the countries used for training with no pre-processing nor\npost-processing needed. Moreover, the library supports fine-tuning with new\ndata to generate a custom address parser.\n","authors":["David Beauchemin","Marouane Yassine"],"pdf_url":"https://arxiv.org/pdf/2311.11846v1.pdf","comment":"Accepted in EMNLP 2024 NLP-OSS workshop. arXiv admin note: text\n  overlap with arXiv:2006.16152, arXiv:2112.04008"},{"id":"http://arxiv.org/abs/2311.11844v1","updated":"2023-11-20T15:34:45Z","published":"2023-11-20T15:34:45Z","title":"How to Use Large Language Models for Text Coding: The Case of Fatherhood\n  Roles in Public Policy Documents","summary":"  Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have\nopened up new opportunities for text analysis in political science. They\npromise automation with better results and less programming. In this study, we\nevaluate LLMs on three original coding tasks of non-English political science\ntexts, and we provide a detailed description of a general workflow for using\nLLMs for text coding in political science research. Our use case offers a\npractical guide for researchers looking to incorporate LLMs into their research\non text analysis. We find that, when provided with detailed label definitions\nand coding examples, an LLM can be as good as or even better than a human\nannotator while being much faster (up to hundreds of times), considerably\ncheaper (costing up to 60% less than human coding), and much easier to scale to\nlarge amounts of text. Overall, LLMs present a viable option for most text\ncoding projects.\n","authors":["Lorenzo Lupo","Oscar Magnusson","Dirk Hovy","Elin Naurin","Lena Wängnerud"],"pdf_url":"https://arxiv.org/pdf/2311.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11829v1","updated":"2023-11-20T15:04:50Z","published":"2023-11-20T15:04:50Z","title":"System 2 Attention (is something you might need too)","summary":"  Soft attention in Transformer-based Large Language Models (LLMs) is\nsusceptible to incorporating irrelevant information from the context into its\nlatent representations, which adversely affects next token generations. To help\nrectify these issues, we introduce System 2 Attention (S2A), which leverages\nthe ability of LLMs to reason in natural language and follow instructions in\norder to decide what to attend to. S2A regenerates the input context to only\ninclude the relevant portions, before attending to the regenerated context to\nelicit the final response. In experiments, S2A outperforms standard\nattention-based LLMs on three tasks containing opinion or irrelevant\ninformation, QA, math word problems and longform generation, where S2A\nincreases factuality and objectivity, and decreases sycophancy.\n","authors":["Jason Weston","Sainbayar Sukhbaatar"],"pdf_url":"https://arxiv.org/pdf/2311.11829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08358v3","updated":"2023-11-20T14:52:43Z","published":"2022-11-15T18:06:53Z","title":"MEAL: Stable and Active Learning for Few-Shot Prompting","summary":"  Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n","authors":["Abdullatif Köksal","Timo Schick","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2211.08358v3.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2311.11813v1","updated":"2023-11-20T14:50:12Z","published":"2023-11-20T14:50:12Z","title":"Efficient Grammatical Error Correction Via Multi-Task Training and\n  Optimized Training Schedule","summary":"  Progress in neural grammatical error correction (GEC) is hindered by the lack\nof annotated training data. Sufficient amounts of high-quality manually\nannotated data are not available, so recent research has relied on generating\nsynthetic data, pretraining on it, and then fine-tuning on real datasets;\nperformance gains have been achieved either by ensembling or by using huge\npretrained models such as XXL-T5 as the backbone. In this work, we explore an\northogonal direction: how to use available data more efficiently. First, we\npropose auxiliary tasks that exploit the alignment between the original and\ncorrected sentences, such as predicting a sequence of corrections. We formulate\neach task as a sequence-to-sequence problem and perform multi-task training.\nSecond, we discover that the order of datasets used for training and even\nindividual instances within a dataset may have important effects on the final\nperformance, so we set out to find the best training schedule. Together, these\ntwo ideas lead to significant improvements, producing results that improve\nstate of the art with much smaller models; in particular, we outperform the\nbest models based on T5-XXL (11B parameters) with a BART-based model (400M\nparameters).\n","authors":["Andrey Bout","Alexander Podolskiy","Sergey Nikolenko","Irina Piontkovskaya"],"pdf_url":"https://arxiv.org/pdf/2311.11813v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.13302v2","updated":"2023-11-20T14:31:26Z","published":"2023-05-22T17:58:01Z","title":"Language-Agnostic Bias Detection in Language Models with Bias Probing","summary":"  Pretrained language models (PLMs) are key components in NLP, but they contain\nstrong social biases. Quantifying these biases is challenging because current\nmethods focusing on fill-the-mask objectives are sensitive to slight changes in\ninput. To address this, we propose a bias probing technique called LABDet, for\nevaluating social bias in PLMs with a robust and language-agnostic method. For\nnationality as a case study, we show that LABDet `surfaces' nationality bias by\ntraining a classifier on top of a frozen PLM on non-nationality sentiment\ndetection. We find consistent patterns of nationality bias across monolingual\nPLMs in six languages that align with historical and political context. We also\nshow for English BERT that bias surfaced by LABDet correlates well with bias in\nthe pretraining data; thus, our work is one of the few studies that directly\nlinks pretraining data to PLM behavior. Finally, we verify LABDet's reliability\nand applicability to different templates and languages through an extensive set\nof robustness checks. We publicly share our code and dataset in\nhttps://github.com/akoksal/LABDet.\n","authors":["Abdullatif Köksal","Omer Faruk Yalcin","Ahmet Akbiyik","M. Tahir Kilavuz","Anna Korhonen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2305.13302v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2311.11797v1","updated":"2023-11-20T14:30:55Z","published":"2023-11-20T14:30:55Z","title":"Igniting Language Intelligence: The Hitchhiker's Guide From\n  Chain-of-Thought Reasoning to Language Agents","summary":"  Large language models (LLMs) have dramatically enhanced the field of language\nintelligence, as demonstrably evidenced by their formidable empirical\nperformance across a spectrum of complex reasoning tasks. Additionally,\ntheoretical proofs have illuminated their emergent reasoning capabilities,\nproviding a compelling showcase of their advanced cognitive abilities in\nlinguistic contexts. Critical to their remarkable efficacy in handling complex\nreasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning\ntechniques, obliging them to formulate intermediate steps en route to deriving\nan answer. The CoT reasoning approach has not only exhibited proficiency in\namplifying reasoning performance but also in enhancing interpretability,\ncontrollability, and flexibility. In light of these merits, recent research\nendeavors have extended CoT reasoning methodologies to nurture the development\nof autonomous language agents, which adeptly adhere to language instructions\nand execute actions within varied environments. This survey paper orchestrates\na thorough discourse, penetrating vital research dimensions, encompassing: (i)\nthe foundational mechanics of CoT techniques, with a focus on elucidating the\ncircumstances and justification behind its efficacy; (ii) the paradigm shift in\nCoT; and (iii) the burgeoning of language agents fortified by CoT approaches.\nProspective research avenues envelop explorations into generalization,\nefficiency, customization, scaling, and safety. This paper caters to a wide\naudience, including beginners seeking comprehensive knowledge of CoT reasoning\nand language agents, as well as experienced researchers interested in\nfoundational mechanics and engaging in cutting-edge discussions on these\ntopics. A repository for the related papers is available at\nhttps://github.com/Zoeyyao27/CoT-Igniting-Agent.\n","authors":["Zhuosheng Zhang","Yao Yao","Aston Zhang","Xiangru Tang","Xinbei Ma","Zhiwei He","Yiming Wang","Mark Gerstein","Rui Wang","Gongshen Liu","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.11797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11796v1","updated":"2023-11-20T14:29:45Z","published":"2023-11-20T14:29:45Z","title":"Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI\n  Systems","summary":"  Artificial Intelligence (AI) systems such as autonomous vehicles, facial\nrecognition, and speech recognition systems are increasingly integrated into\nour daily lives. However, despite their utility, these AI systems are\nvulnerable to a wide range of attacks such as adversarial, backdoor, data\npoisoning, membership inference, model inversion, and model stealing attacks.\nIn particular, numerous attacks are designed to target a particular model or\nsystem, yet their effects can spread to additional targets, referred to as\ntransferable attacks. Although considerable efforts have been directed toward\ndeveloping transferable attacks, a holistic understanding of the advancements\nin transferable attacks remains elusive. In this paper, we comprehensively\nexplore learning-based attacks from the perspective of transferability,\nparticularly within the context of cyber-physical security. We delve into\ndifferent domains -- the image, text, graph, audio, and video domains -- to\nhighlight the ubiquitous and pervasive nature of transferable attacks. This\npaper categorizes and reviews the architecture of existing attacks from various\nviewpoints: data, process, model, and system. We further examine the\nimplications of transferable attacks in practical scenarios such as autonomous\ndriving, speech recognition, and large language models (LLMs). Additionally, we\noutline the potential research directions to encourage efforts in exploring the\nlandscape of transferable attacks. This survey offers a holistic understanding\nof the prevailing transferable attacks and their impacts across different\ndomains.\n","authors":["Guangjing Wang","Ce Zhou","Yuanda Wang","Bocheng Chen","Hanqing Guo","Qiben Yan"],"pdf_url":"https://arxiv.org/pdf/2311.11796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15363v4","updated":"2023-11-20T13:59:16Z","published":"2023-08-29T14:59:54Z","title":"Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation","summary":"  Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL\ntask. However, the absence of a systematical benchmark inhibits the development\nof designing effective, efficient and economic LLM-based Text-to-SQL solutions.\nTo address this challenge, in this paper, we first conduct a systematical and\nextensive comparison over existing prompt engineering methods, including\nquestion representation, example selection and example organization, and with\nthese experimental results, we elaborate their pros and cons. Based on these\nfindings, we propose a new integrated solution, named DAIL-SQL, which refreshes\nthe Spider leaderboard with 86.6% execution accuracy and sets a new bar. To\nexplore the potential of open-source LLM, we investigate them in various\nscenarios, and further enhance their performance with supervised fine-tuning.\nOur explorations highlight open-source LLMs' potential in Text-to-SQL, as well\nas the advantages and disadvantages of the supervised fine-tuning.\nAdditionally, towards an efficient and economic LLM-based Text-to-SQL solution,\nwe emphasize the token efficiency in prompt engineering and compare the prior\nstudies under this metric. We hope that our work provides a deeper\nunderstanding of Text-to-SQL with LLMs, and inspires further investigations and\nbroad applications.\n","authors":["Dawei Gao","Haibin Wang","Yaliang Li","Xiuyu Sun","Yichen Qian","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.15363v4.pdf","comment":"We have released code on https://github.com/BeachWang/DAIL-SQL"},{"id":"http://arxiv.org/abs/2309.10003v3","updated":"2023-11-20T13:31:47Z","published":"2023-09-17T16:50:07Z","title":"A novel approach to measuring patent claim scope based on probabilities\n  obtained from (large) language models","summary":"  This work proposes to measure the scope of a patent claim as the reciprocal\nof the self-information contained in this claim. A probability of occurrence of\nthe claim is obtained from a language model and this probability is used to\ncompute the self-information. Grounded in information theory, this approach is\nbased on the assumption that an unlikely concept is more informative than a\nusual concept, insofar as it is more surprising. In turn, the more surprising\nthe information required to defined the claim, the narrower its scope. Five\nlanguage models are considered, ranging from simplest models (each word or\ncharacter is assigned an identical probability) to intermediate models (using\naverage word or character frequencies), to a large language model (GPT2).\nInterestingly, the scope resulting from the simplest language models is\nproportional to the reciprocal of the number of words or characters involved in\nthe claim, a metric already used in previous works. Application is made to\nmultiple series of patent claims directed to distinct inventions, where each\nseries consists of claims devised to have a gradually decreasing scope. The\nperformance of the language models is assessed with respect to several ad hoc\ntests. The more sophisticated the model, the better the results. I.e., the GPT2\nprobability model outperforms models based on word and character frequencies,\nwhich themselves outdo the simplest models based on word or character counts.\nStill, the character count appears to be a more reliable indicator than the\nword count.\n","authors":["Sébastien Ragot"],"pdf_url":"https://arxiv.org/pdf/2309.10003v3.pdf","comment":"58 pages, 8 tables, 6 figures. Substantial changes made to version 2:\n  New section 4.1 added (including a new table); Minor normalization issue\n  corrected in values listed in Appendix B; Content of former appendix C now\n  moved to Section 3; and new Appendix C added. Minor changes made to version 3\n  (style, typos, language)"},{"id":"http://arxiv.org/abs/2311.11745v1","updated":"2023-11-20T13:13:24Z","published":"2023-11-20T13:13:24Z","title":"Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis","summary":"  In this work, we propose a novel method for modeling numerous speakers, which\nenables expressing the overall characteristics of speakers in detail like a\ntrained multi-speaker model without additional training on the target speaker's\ndataset. Although various works with similar purposes have been actively\nstudied, their performance has not yet reached that of trained multi-speaker\nmodels due to their fundamental limitations. To overcome previous limitations,\nwe propose effective methods for feature learning and representing target\nspeakers' speech characteristics by discretizing the features and conditioning\nthem to a speech synthesis model. Our method obtained a significantly higher\nsimilarity mean opinion score (SMOS) in subjective similarity evaluation than\nseen speakers of a best-performing multi-speaker model, even with unseen\nspeakers. The proposed method also outperforms a zero-shot method by\nsignificant margins. Furthermore, our method shows remarkable performance in\ngenerating new artificial speakers. In addition, we demonstrate that the\nencoded latent features are sufficiently informative to reconstruct an original\nspeaker's speech completely. It implies that our method can be used as a\ngeneral methodology to encode and reconstruct speakers' characteristics in\nvarious tasks.\n","authors":["Jungil Kong","Junmo Lee","Jeongmin Kim","Beomjeong Kim","Jihoon Park","Dohee Kong","Changheon Lee","Sangjin Kim"],"pdf_url":"https://arxiv.org/pdf/2311.11745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19130v2","updated":"2023-11-20T12:15:19Z","published":"2023-10-29T19:39:03Z","title":"Women Wearing Lipstick: Measuring the Bias Between an Object and Its\n  Related Gender","summary":"  In this paper, we investigate the impact of objects on gender bias in image\ncaptioning systems. Our results show that only gender-specific objects have a\nstrong gender bias (e.g., women-lipstick). In addition, we propose a visual\nsemantic-based gender score that measures the degree of bias and can be used as\na plug-in for any image captioning system. Our experiments demonstrate the\nutility of the gender score, since we observe that our score can measure the\nbias relation between a caption and its related gender; therefore, our score\ncan be used as an additional metric to the existing Object Gender Co-Occ\napproach. Code and data are publicly available at\n\\url{https://github.com/ahmedssabir/GenderScore}.\n","authors":["Ahmed Sabir","Lluís Padró"],"pdf_url":"https://arxiv.org/pdf/2310.19130v2.pdf","comment":"EMNLP Findings 2023"},{"id":"http://arxiv.org/abs/2311.11701v1","updated":"2023-11-20T12:08:32Z","published":"2023-11-20T12:08:32Z","title":"Control in Hybrid Chatbots","summary":"  Customer data typically is held in database systems, which can be seen as\nrule-based knowledge base, whereas businesses increasingly want to benefit from\nthe capabilities of large, pre-trained language models.\n  In this technical report, we describe a case study of how a commercial rule\nengine and an integrated neural chatbot may be integrated, and what level of\ncontrol that particular integration mode leads to. We also discuss alternative\nways (including past ways realized in other systems) how researchers strive to\nmaintain control and avoid what has recently been called model \"hallucination\".\n","authors":["Thomas Rüdel","Jochen L. Leidner"],"pdf_url":"https://arxiv.org/pdf/2311.11701v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.11696v1","updated":"2023-11-20T11:56:25Z","published":"2023-11-20T11:56:25Z","title":"Sparse Low-rank Adaptation of Pre-trained Language Models","summary":"  Fine-tuning pre-trained large language models in a parameter-efficient manner\nis widely studied for its effectiveness and efficiency. The popular method of\nlow-rank adaptation (LoRA) offers a notable approach, hypothesizing that the\nadaptation process is intrinsically low-dimensional. Although LoRA has\ndemonstrated commendable performance, it is implemented with a fixed and\nunalterable intrinsic rank that might not always be the ideal choice.\nRecognizing the need for more flexible adaptation, we extend the methodology of\nLoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that\nenables dynamic adjustments to the intrinsic rank during the adaptation\nprocess. We achieve this through the incorporation of a gate unit optimized\nwith proximal gradient method in the training stage, controlling the\ncardinality of rank under the sparsity of the gate. In the subsequent inference\nstage, we eliminate the parameter blocks corresponding to the zeroed-out ranks,\nto reduce each SoRA module back to a concise yet rank-optimal LoRA. Our\napproach strengthens the representation power of LoRA by initializing it with a\nhigher rank, while efficiently taming a temporarily increased number of\nparameters via updating in a sparse way. We further introduce a sparsifying\nscheduler for SoRA, aiming to examine the impact of the number of non-zero\nparameters on the model's memorization and generalization. Our experimental\nresults demonstrate that SoRA can outperform other baselines even with 70%\nretained parameters and 70% training time.\n","authors":["Ning Ding","Xingtai Lv","Qiaosen Wang","Yulin Chen","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2311.11696v1.pdf","comment":"Accepted to EMNLP 2023 (Main Conference)"},{"id":"http://arxiv.org/abs/2311.11690v1","updated":"2023-11-20T11:43:45Z","published":"2023-11-20T11:43:45Z","title":"Refactoring Programs Using Large Language Models with Few-Shot Examples","summary":"  A less complex and more straightforward program is a crucial factor that\nenhances its maintainability and makes writing secure and bug-free programs\neasier. However, due to its heavy workload and the risks of breaking the\nworking programs, programmers are reluctant to do code refactoring, and thus,\nit also causes the loss of potential learning experiences. To mitigate this, we\ndemonstrate the application of using a large language model (LLM), GPT-3.5, to\nsuggest less complex versions of the user-written Python program, aiming to\nencourage users to learn how to write better programs. We propose a method to\nleverage the prompting with few-shot examples of the LLM by selecting the\nbest-suited code refactoring examples for each target programming problem based\non the prior evaluation of prompting with the one-shot example. The\nquantitative evaluation shows that 95.68% of programs can be refactored by\ngenerating 10 candidates each, resulting in a 17.35% reduction in the average\ncyclomatic complexity and a 25.84% decrease in the average number of lines\nafter filtering only generated programs that are semantically correct.\nFurthermore, the qualitative evaluation shows outstanding capability in code\nformatting, while unnecessary behaviors such as deleting or translating\ncomments are also observed.\n","authors":["Atsushi Shirafuji","Yusuke Oda","Jun Suzuki","Makoto Morishita","Yutaka Watanobe"],"pdf_url":"https://arxiv.org/pdf/2311.11690v1.pdf","comment":"10 pages, 10 figures, accepted to the 30th Asia-Pacific Software\n  Engineering Conference (APSEC 2023)"},{"id":"http://arxiv.org/abs/2310.10348v2","updated":"2023-11-20T11:31:16Z","published":"2023-10-16T12:34:43Z","title":"Attribution Patching Outperforms Automated Circuit Discovery","summary":"  Automated interpretability research has recently attracted attention as a\npotential research direction that could scale explanations of neural network\nbehavior to large models. Existing automated circuit discovery work applies\nactivation patching to identify subnetworks responsible for solving specific\ntasks (circuits). In this work, we show that a simple method based on\nattribution patching outperforms all existing methods while requiring just two\nforward passes and a backward pass. We apply a linear approximation to\nactivation patching to estimate the importance of each edge in the\ncomputational subgraph. Using this approximation, we prune the least important\nedges of the network. We survey the performance and limitations of this method,\nfinding that averaged over all tasks our method has greater AUC from circuit\nrecovery than other methods.\n","authors":["Aaquib Syed","Can Rager","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2310.10348v2.pdf","comment":"6 main paper pages, 6 additional pages. NeurIPS 2023 ATTRIB Workshop"},{"id":"http://arxiv.org/abs/2311.11608v1","updated":"2023-11-20T08:51:30Z","published":"2023-11-20T08:51:30Z","title":"Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse\n  Biomedical Tasks","summary":"  Recent advancements in large language models (LLMs) have shown promising\nresults across a variety of natural language processing (NLP) tasks. The\napplication of LLMs to specific domains, such as biomedicine, has achieved\nincreased attention. However, most biomedical LLMs focus on enhancing\nperformance in monolingual biomedical question answering and conversation\ntasks. To further investigate the effectiveness of the LLMs on diverse\nbiomedical NLP tasks in different languages, we present Taiyi, a bilingual\n(English and Chinese) fine-tuned LLM for diverse biomedical tasks. In this\nwork, we first curated a comprehensive collection of 140 existing biomedical\ntext mining datasets across over 10 task types. Subsequently, a two-stage\nstrategy is proposed for supervised fine-tuning to optimize the model\nperformance across varied tasks. Experimental results on 13 test sets covering\nnamed entity recognition, relation extraction, text classification, question\nanswering tasks demonstrate Taiyi achieves superior performance compared to\ngeneral LLMs. The case study involving additional biomedical NLP tasks further\nshows Taiyi's considerable potential for bilingual biomedical multi-tasking.\nThe source code, datasets, and model for Taiyi are freely available at\nhttps://github.com/DUTIR-BioNLP/Taiyi-LLM.\n","authors":["Ling Luo","Jinzhong Ning","Yingwen Zhao","Zhijun Wang","Zeyuan Ding","Peng Chen","Weiru Fu","Qinyu Han","Guangtao Xu","Yunzhi Qiu","Dinghao Pan","Jiru Li","Hao Li","Wenduo Feng","Senbo Tu","Yuqi Liu","Zhihao Yang","Jian Wang","Yuanyuan Sun","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2311.11608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11601v1","updated":"2023-11-20T08:29:52Z","published":"2023-11-20T08:29:52Z","title":"Addressing the Length Bias Problem in Document-Level Neural Machine\n  Translation","summary":"  Document-level neural machine translation (DNMT) has shown promising results\nby incorporating more context information. However, this approach also\nintroduces a length bias problem, whereby DNMT suffers from significant\ntranslation quality degradation when decoding documents that are much shorter\nor longer than the maximum sequence length during training. %i.e., the length\nbias problem. To solve the length bias problem, we propose to improve the DNMT\nmodel in training method, attention mechanism, and decoding strategy. Firstly,\nwe propose to sample the training data dynamically to ensure a more uniform\ndistribution across different sequence lengths. Then, we introduce a\nlength-normalized attention mechanism to aid the model in focusing on target\ninformation, mitigating the issue of attention divergence when processing\nlonger sequences. Lastly, we propose a sliding window strategy during decoding\nthat integrates as much context information as possible without exceeding the\nmaximum sequence length. The experimental results indicate that our method can\nbring significant improvements on several open datasets, and further analysis\nshows that our method can significantly alleviate the length bias problem.\n","authors":["Zhuocheng Zhang","Shuhao Gu","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2311.11601v1.pdf","comment":"Accepted by EMNLP2023 Findings"},{"id":"http://arxiv.org/abs/2311.11598v1","updated":"2023-11-20T08:23:39Z","published":"2023-11-20T08:23:39Z","title":"Filling the Image Information Gap for VQA: Prompting Large Language\n  Models to Proactively Ask Questions","summary":"  Large Language Models (LLMs) demonstrate impressive reasoning ability and the\nmaintenance of world knowledge not only in natural language tasks, but also in\nsome vision-language tasks such as open-domain knowledge-based visual question\nanswering (OK-VQA). As images are invisible to LLMs, researchers convert images\nto text to engage LLMs into the visual question reasoning procedure. This leads\nto discrepancies between images and their textual representations presented to\nLLMs, which consequently impedes final reasoning performance. To fill the\ninformation gap and better leverage the reasoning capability, we design a\nframework that enables LLMs to proactively ask relevant questions to unveil\nmore details in the image, along with filters for refining the generated\ninformation. We validate our idea on OK-VQA and A-OKVQA. Our method\ncontinuously boosts the performance of baselines methods by an average gain of\n2.15% on OK-VQA, and achieves consistent improvements across different LLMs.\n","authors":["Ziyue Wang","Chi Chen","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.11598v1.pdf","comment":"Accepted to EMNLP2023 Findings"},{"id":"http://arxiv.org/abs/2311.11583v1","updated":"2023-11-20T07:41:30Z","published":"2023-11-20T07:41:30Z","title":"How well ChatGPT understand Malaysian English? An Evaluation on Named\n  Entity Recognition and Relation Extraction","summary":"  Recently, ChatGPT has attracted a lot of interest from both researchers and\nthe general public. While the performance of ChatGPT in named entity\nrecognition and relation extraction from Standard English texts is\nsatisfactory, it remains to be seen if it can perform similarly for Malaysian\nEnglish. Malaysian English is unique as it exhibits morphosyntactic and\nsemantical adaptation from local contexts. In this study, we assess ChatGPT's\ncapability in extracting entities and relations from the Malaysian English News\n(MEN) dataset. We propose a three-step methodology referred to as\n\\textbf{\\textit{educate-predict-evaluate}}. The performance of ChatGPT is\nassessed using F1-Score across 18 unique prompt settings, which were carefully\nengineered for a comprehensive review. From our evaluation, we found that\nChatGPT does not perform well in extracting entities from Malaysian English\nnews articles, with the highest F1-Score of 0.497. Further analysis shows that\nthe morphosyntactic adaptation in Malaysian English caused the limitation.\nHowever, interestingly, this morphosyntactic adaptation does not impact the\nperformance of ChatGPT for relation extraction.\n","authors":["Mohan Raj Chanthran","Lay-Ki Soon","Huey Fang Ong","Bhawani Selvaretnam"],"pdf_url":"https://arxiv.org/pdf/2311.11583v1.pdf","comment":"Accepted in Generation, Evaluation & Metrics (GEM) Workshop at EMNLP\n  2023"},{"id":"http://arxiv.org/abs/2311.11564v1","updated":"2023-11-20T07:02:35Z","published":"2023-11-20T07:02:35Z","title":"KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained\n  Language Model","summary":"  Most biomedical pretrained language models are monolingual and cannot handle\nthe growing cross-lingual requirements. The scarcity of non-English domain\ncorpora, not to mention parallel data, poses a significant hurdle in training\nmultilingual biomedical models. Since knowledge forms the core of\ndomain-specific corpora and can be translated into various languages\naccurately, we propose a model called KBioXLM, which transforms the\nmultilingual pretrained model XLM-R into the biomedical domain using a\nknowledge-anchored approach. We achieve a biomedical multilingual corpus by\nincorporating three granularity knowledge alignments (entity, fact, and passage\nlevels) into monolingual corpora. Then we design three corresponding training\ntasks (entity masking, relation masking, and passage relation prediction) and\ncontinue training on top of the XLM-R model to enhance its domain cross-lingual\nability. To validate the effectiveness of our model, we translate the English\nbenchmarks of multiple tasks into Chinese. Experimental results demonstrate\nthat our model significantly outperforms monolingual and multilingual\npretrained models in cross-lingual zero-shot and few-shot scenarios, achieving\nimprovements of up to 10+ points. Our code is publicly available at\nhttps://github.com/ngwlh-gl/KBioXLM.\n","authors":["Lei Geng","Xu Yan","Ziqiang Cao","Juntao Li","Wenjie Li","Sujian Li","Xinjie Zhou","Yang Yang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.11564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11552v1","updated":"2023-11-20T06:06:22Z","published":"2023-11-20T06:06:22Z","title":"Exploring Prompting Large Language Models as Explainable Metrics","summary":"  This paper describes the IUST NLP Lab submission to the Prompting Large\nLanguage Models as Explainable Metrics Shared Task at the Eval4NLP 2023\nWorkshop on Evaluation & Comparison of NLP Systems. We have proposed a\nzero-shot prompt-based strategy for explainable evaluation of the summarization\ntask using Large Language Models (LLMs). The conducted experiments demonstrate\nthe promising potential of LLMs as evaluation metrics in Natural Language\nProcessing (NLP), particularly in the field of summarization. Both few-shot and\nzero-shot approaches are employed in these experiments. The performance of our\nbest provided prompts achieved a Kendall correlation of 0.477 with human\nevaluations in the text summarization task on the test data. Code and results\nare publicly available on GitHub.\n","authors":["Ghazaleh Mahmoudi"],"pdf_url":"https://arxiv.org/pdf/2311.11552v1.pdf","comment":"9 pages, Eval4NLP 2023"},{"id":"http://arxiv.org/abs/2311.11551v1","updated":"2023-11-20T06:06:20Z","published":"2023-11-20T06:06:20Z","title":"Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context\n  Learning","summary":"  Large language models (LLMs) have showcased their capability with few-shot\ninference known as in-context learning. However, in-domain demonstrations are\nnot always readily available in real scenarios, leading to cross-domain\nin-context learning. Besides, LLMs are still facing challenges in long-tail\nknowledge in unseen and unfamiliar domains. The above limitations demonstrate\nthe necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study\nthe UDA problem under an in-context learning setting to adapt language models\nfrom the source domain to the target domain without any target labels. The core\nidea is to retrieve a subset of cross-domain elements that are the most similar\nto the query, and elicit language model to adapt in an in-context manner by\nlearning both target domain distribution and the discriminative task signal\nsimultaneously with the augmented cross-domain in-context examples. We devise\ndifferent prompting and training strategies, accounting for different LM\narchitectures to learn the target distribution via language modeling. With\nextensive experiments on Sentiment Analysis (SA) and Named Entity Recognition\n(NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer\nand demonstrate significant improvements over baseline models.\n","authors":["Quanyu Long","Wenya Wang","Sinno Jialin Pan"],"pdf_url":"https://arxiv.org/pdf/2311.11551v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2205.15439v2","updated":"2023-11-20T04:31:13Z","published":"2022-05-30T21:34:40Z","title":"StyleTTS: A Style-Based Generative Model for Natural and Diverse\n  Text-to-Speech Synthesis","summary":"  Text-to-Speech (TTS) has recently seen great progress in synthesizing\nhigh-quality speech owing to the rapid development of parallel TTS systems, but\nproducing speech with naturalistic prosodic variations, speaking styles and\nemotional tones remains challenging. Moreover, since duration and speech are\ngenerated separately, parallel TTS models still have problems finding the best\nmonotonic alignments that are crucial for naturalistic speech synthesis. Here,\nwe propose StyleTTS, a style-based generative model for parallel TTS that can\nsynthesize diverse speech with natural prosody from a reference speech\nutterance. With novel Transferable Monotonic Aligner (TMA) and\nduration-invariant data augmentation schemes, our method significantly\noutperforms state-of-the-art models on both single and multi-speaker datasets\nin subjective tests of speech naturalness and speaker similarity. Through\nself-supervised learning of the speaking styles, our model can synthesize\nspeech with the same prosodic and emotional tone as any given reference speech\nwithout the need for explicitly labeling these categories.\n","authors":["Yinghao Aaron Li","Cong Han","Nima Mesgarani"],"pdf_url":"https://arxiv.org/pdf/2205.15439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07691v2","updated":"2023-11-20T04:23:08Z","published":"2023-06-13T11:04:43Z","title":"StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion\n  and Adversarial Training with Large Speech Language Models","summary":"  In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that\nleverages style diffusion and adversarial training with large speech language\nmodels (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its\npredecessor by modeling styles as a latent random variable through diffusion\nmodels to generate the most suitable style for the text without requiring\nreference speech, achieving efficient latent diffusion while benefiting from\nthe diverse speech synthesis offered by diffusion models. Furthermore, we\nemploy large pre-trained SLMs, such as WavLM, as discriminators with our novel\ndifferentiable duration modeling for end-to-end training, resulting in improved\nspeech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker\nLJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by\nnative English speakers. Moreover, when trained on the LibriTTS dataset, our\nmodel outperforms previous publicly available models for zero-shot speaker\nadaptation. This work achieves the first human-level TTS on both single and\nmultispeaker datasets, showcasing the potential of style diffusion and\nadversarial training with large SLMs. The audio demos and source code are\navailable at https://styletts2.github.io/.\n","authors":["Yinghao Aaron Li","Cong Han","Vinay S. Raghavan","Gavin Mischler","Nima Mesgarani"],"pdf_url":"https://arxiv.org/pdf/2306.07691v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2304.13867v3","updated":"2023-11-20T04:09:25Z","published":"2023-04-26T23:24:50Z","title":"Transferring Procedural Knowledge across Commonsense Tasks","summary":"  Stories about everyday situations are an essential part of human\ncommunication, motivating the need to develop AI agents that can reliably\nunderstand these stories. Despite the long list of supervised methods for story\ncompletion and procedural understanding, current AI has no mechanisms to\nautomatically track and explain procedures in unseen stories. To bridge this\ngap, we study the ability of AI models to transfer procedural knowledge to\nnovel narrative tasks in a transparent manner. We design LEAP: a comprehensive\nframework that integrates state-of-the-art modeling architectures, training\nregimes, and augmentation strategies based on both natural and synthetic\nstories. To address the lack of densely annotated training data, we devise a\nrobust automatic labeler based on few-shot prompting to enhance the augmented\ndata. Our experiments with in- and out-of-domain tasks reveal insights into the\ninterplay of different architectures, training regimes, and augmentation\nstrategies. LEAP's labeler has a clear positive impact on out-of-domain\ndatasets, while the resulting dense annotation provides native explainability.\n","authors":["Yifan Jiang","Filip Ilievski","Kaixin Ma"],"pdf_url":"https://arxiv.org/pdf/2304.13867v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11518v1","updated":"2023-11-20T03:44:32Z","published":"2023-11-20T03:44:32Z","title":"Multi-teacher Distillation for Multilingual Spelling Correction","summary":"  Accurate spelling correction is a critical step in modern search interfaces,\nespecially in an era of mobile devices and speech-to-text interfaces. For\nservices that are deployed around the world, this poses a significant challenge\nfor multilingual NLP: spelling errors need to be caught and corrected in all\nlanguages, and even in queries that use multiple languages. In this paper, we\ntackle this challenge using multi-teacher distillation. On our approach, a\nmonolingual teacher model is trained for each language/locale, and these\nindividual models are distilled into a single multilingual student model\nintended to serve all languages/locales. In experiments using open-source data\nas well as user data from a worldwide search service, we show that this leads\nto highly effective spelling correction models that can meet the tight latency\nrequirements of deployed services.\n","authors":["Jingfen Zhang","Xuan Guo","Sravan Bodapati","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2311.11518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11516v1","updated":"2023-11-20T03:42:24Z","published":"2023-11-20T03:42:24Z","title":"GPT in Data Science: A Practical Exploration of Model Selection","summary":"  There is an increasing interest in leveraging Large Language Models (LLMs)\nfor managing structured data and enhancing data science processes. Despite the\npotential benefits, this integration poses significant questions regarding\ntheir reliability and decision-making methodologies. It highlights the\nimportance of various factors in the model selection process, including the\nnature of the data, problem type, performance metrics, computational resources,\ninterpretability vs accuracy, assumptions about data, and ethical\nconsiderations. Our objective is to elucidate and express the factors and\nassumptions guiding GPT-4's model selection recommendations. We employ a\nvariability model to depict these factors and use toy datasets to evaluate both\nthe model and the implementation of the identified heuristics. By contrasting\nthese outcomes with heuristics from other platforms, our aim is to determine\nthe effectiveness and distinctiveness of GPT-4's methodology. This research is\ncommitted to advancing our comprehension of AI decision-making processes,\nespecially in the realm of model selection within data science. Our efforts are\ndirected towards creating AI systems that are more transparent and\ncomprehensible, contributing to a more responsible and efficient practice in\ndata science.\n","authors":["Nathalia Nascimento","Cristina Tavares","Paulo Alencar","Donald Cowan"],"pdf_url":"https://arxiv.org/pdf/2311.11516v1.pdf","comment":"11 pages. To appear in IEEE BigData 2023"},{"id":"http://arxiv.org/abs/2310.09590v2","updated":"2023-11-20T03:29:23Z","published":"2023-10-14T14:23:44Z","title":"Solving Math Word Problems with Reexamination","summary":"  Math word problem (MWP) solving aims to understand the descriptive math\nproblem and calculate the result, for which previous efforts are mostly devoted\nto upgrade different technical modules. This paper brings a different\nperspective of \\textit{reexamination process} during training by introducing a\npseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual)\nlearning scheme to model such process, which is model-agnostic thus can be\nadapted to any existing MWP solvers. The pseudo-dual task is specifically\ndefined as filling the numbers in the expression back into the original word\nproblem with numbers masked. To facilitate the effective joint learning of the\ntwo tasks, we further design a scheduled fusion strategy for the number\ninfilling task, which smoothly switches the input from the ground-truth math\nexpressions to the predicted ones. Our pseudo-dual learning scheme has been\ntested and proven effective when being equipped in several representative MWP\nsolvers through empirical studies. \\textit{The codes and trained models are\navailable at:} \\url{https://github.com/steven640pixel/PsedualMWP}.\n\\end{abstract}\n","authors":["Yi Bin","Wenhao Shi","Yujuan Ding","Yang Yang","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2310.09590v2.pdf","comment":"To be appeared at NeurIPS2023 Workshop on MATH-AI"},{"id":"http://arxiv.org/abs/2311.11509v1","updated":"2023-11-20T03:17:21Z","published":"2023-11-20T03:17:21Z","title":"Token-Level Adversarial Prompt Detection Based on Perplexity Measures\n  and Contextual Information","summary":"  In recent years, Large Language Models (LLM) have emerged as pivotal tools in\nvarious applications. However, these models are susceptible to adversarial\nprompt attacks, where attackers can carefully curate input strings that lead to\nundesirable outputs. The inherent vulnerability of LLMs stems from their\ninput-output mechanisms, especially when presented with intensely\nout-of-distribution (OOD) inputs. This paper proposes a token-level detection\nmethod to identify adversarial prompts, leveraging the LLM's capability to\npredict the next token's probability. We measure the degree of the model's\nperplexity and incorporate neighboring token information to encourage the\ndetection of contiguous adversarial prompt sequences. As a result, we propose\ntwo methods: one that identifies each token as either being part of an\nadversarial prompt or not, and another that estimates the probability of each\ntoken being part of an adversarial prompt.\n","authors":["Zhengmian Hu","Gang Wu","Saayan Mitra","Ruiyi Zhang","Tong Sun","Heng Huang","Vishy Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2311.11509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10702v2","updated":"2023-11-20T02:01:33Z","published":"2023-11-17T18:45:45Z","title":"Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2","summary":"  Since the release of T\\\"ULU [Wang et al., 2023b], open resources for\ninstruction tuning have developed quickly, from better base models to new\nfinetuning techniques. We test and incorporate a number of these advances into\nT\\\"ULU, resulting in T\\\"ULU 2, a suite of improved T\\\"ULU models for advancing\nthe understanding and best practices of adapting pretrained language models to\ndownstream tasks and user preferences. Concretely, we release: (1)\nT\\\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2)\nT\\\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\\\"ULU 2+DPO, T\\\"ULU\n2 models trained with direct preference optimization (DPO), including the\nlargest DPO-trained model to date (T\\\"ULU 2+DPO 70B); (4) CODE T\\\"ULU 2, CODE\nLLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its\ninstruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple\nperspectives shows that the T\\\"ULU 2 suite achieves state-of-the-art\nperformance among open models and matches or exceeds the performance of\nGPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data,\ntraining and evaluation code to facilitate future open efforts on adapting\nlarge language models.\n","authors":["Hamish Ivison","Yizhong Wang","Valentina Pyatkin","Nathan Lambert","Matthew Peters","Pradeep Dasigi","Joel Jang","David Wadden","Noah A. Smith","Iz Beltagy","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2311.10702v2.pdf","comment":"technical report; fixed zephyr numbers"},{"id":"http://arxiv.org/abs/2311.11482v1","updated":"2023-11-20T01:51:13Z","published":"2023-11-20T01:51:13Z","title":"Meta Prompting for AGI Systems","summary":"  This paper presents an in-depth exploration of Meta Prompting, a novel\ntechnique that revolutionizes the way large language models (LLMs), multi-modal\nfoundation models, and AI systems approach problem-solving and data\ninterpretation. Meta Prompting, rooted in type theory and category theory,\nprioritizes the structure and syntax of information, providing a unique\nframework that transcends traditional content-focused methods. We delve into\nthe formal definitions of Meta Prompting, contrasting it with Few-Shot\nPrompting, and highlight its applicability and superiority in various AI\napplications.\n  Key to this exploration is the expansion of Meta Prompting into the realm of\ncomplex reasoning. Here, we demonstrate how this technique adeptly breaks down\nintricate problems into manageable sub-problems, facilitating a step-by-step,\ndetailed approach to problem-solving. This method proves especially\nadvantageous in terms of token efficiency and offering a fair comparison in\nproblem-solving scenarios, standing out against few-shot example approaches.\n  Furthermore, the paper breaks new ground by extending Meta Prompting into\nmulti-modal foundation model settings. This extension addresses the integration\nof diverse data types, such as images, audio, and video, within the structured\nframework of Meta Prompting, highlighting both the challenges and the vast\npotential of this approach in handling complex, multi-faceted data (The code is\navailable at https://github.com/meta-prompting/meta-prompting).\n","authors":["Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.11482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05619v2","updated":"2023-11-20T01:24:40Z","published":"2023-09-11T17:07:01Z","title":"Effective Proxy for Human Labeling: Ensemble Disagreement Scores in\n  Large Language Models for Industrial NLP","summary":"  Large language models (LLMs) have demonstrated significant capability to\ngeneralize across a large number of NLP tasks. For industry applications, it is\nimperative to assess the performance of the LLM on unlabeled production data\nfrom time to time to validate for a real-world setting. Human labeling to\nassess model error requires considerable expense and time delay. Here we\ndemonstrate that ensemble disagreement scores work well as a proxy for human\nlabeling for language models in zero-shot, few-shot, and fine-tuned settings,\nper our evaluation on keyphrase extraction (KPE) task. We measure fidelity of\nthe results by comparing to true error measured from human labeled ground\ntruth. We contrast with the alternative of using another LLM as a source of\nmachine labels, or silver labels. Results across various languages and domains\nshow disagreement scores provide a better estimation of model performance with\nmean average error (MAE) as low as 0.4% and on average 13.8% better than using\nsilver labels.\n","authors":["Wei Du","Laksh Advani","Yashmeet Gambhir","Daniel J Perry","Prashant Shiralkar","Zhengzheng Xing","Aaron Colak"],"pdf_url":"https://arxiv.org/pdf/2309.05619v2.pdf","comment":"Camera ready version for 2023 EMNLP (The Third Workshop on Natural\n  Language Generation, Evaluation, and Metrics (GEM))"},{"id":"http://arxiv.org/abs/2305.16300v2","updated":"2023-11-20T01:16:17Z","published":"2023-05-25T17:53:42Z","title":"Landmark Attention: Random-Access Infinite Context Length for\n  Transformers","summary":"  While Transformers have shown remarkable success in natural language\nprocessing, their attention mechanism's large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model's attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system's memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity to over 32k tokens, allowing\nfor inference at the context lengths of GPT-4. We release the implementation of\nlandmark attention and the code to reproduce our experiments at\nhttps://github.com/epfml/landmark-attention/.\n","authors":["Amirkeivan Mohtashami","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2305.16300v2.pdf","comment":"Published as a conference paper at NeurIPS 2023 - 37th Conference on\n  Neural Information Processing Systems"},{"id":"http://arxiv.org/abs/2311.11477v1","updated":"2023-11-20T01:07:30Z","published":"2023-11-20T01:07:30Z","title":"What's left can't be right -- The remaining positional incompetence of\n  contrastive vision-language models","summary":"  Contrastive vision-language models like CLIP have been found to lack spatial\nunderstanding capabilities. In this paper we discuss the possible causes of\nthis phenomenon by analysing both datasets and embedding space. By focusing on\nsimple left-right positional relations, we show that this behaviour is entirely\npredictable, even with large-scale datasets, demonstrate that these relations\ncan be taught using synthetic data and show that this approach can generalise\nwell to natural images - improving the performance on left-right relations on\nVisual Genome Relations.\n","authors":["Nils Hoehing","Ellen Rushe","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2311.11477v1.pdf","comment":null}]},"2023-11-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.11462v1","updated":"2023-11-19T23:59:22Z","published":"2023-11-19T23:59:22Z","title":"LLM aided semi-supervision for Extractive Dialog Summarization","summary":"  Generating high-quality summaries for chat dialogs often requires large\nlabeled datasets. We propose a method to efficiently use unlabeled data for\nextractive summarization of customer-agent dialogs. In our method, we frame\nsummarization as a question-answering problem and use state-of-the-art large\nlanguage models (LLMs) to generate pseudo-labels for a dialog. We then use\nthese pseudo-labels to fine-tune a chat summarization model, effectively\ntransferring knowledge from the large LLM into a smaller specialized model. We\ndemonstrate our method on the \\tweetsumm dataset, and show that using 10\\% of\nthe original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L,\nwhereas the current state-of-the-art trained on the entire training data set\nobtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case\n(i.e., ROUGE-L) we still effectively retain 94.7% of the performance while\nusing only 10% of the data.\n","authors":["Nishant Mishra","Gaurav Sahu","Iacer Calixto","Ameen Abu-Hanna","Issam H. Laradji"],"pdf_url":"https://arxiv.org/pdf/2311.11462v1.pdf","comment":"to be published in EMNLP Findings"},{"id":"http://arxiv.org/abs/2311.11441v1","updated":"2023-11-19T22:29:15Z","published":"2023-11-19T22:29:15Z","title":"Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using\n  Clustering and Information Theory Techniques","summary":"  With the development of generative models like GPT-3, it is increasingly more\nchallenging to differentiate generated texts from human-written ones. There is\na large number of studies that have demonstrated good results in bot\nidentification. However, the majority of such works depend on supervised\nlearning methods that require labelled data and/or prior knowledge about the\nbot-model architecture. In this work, we propose a bot identification algorithm\nthat is based on unsupervised learning techniques and does not depend on a\nlarge amount of labelled data. By combining findings in semantic analysis by\nclustering (crisp and fuzzy) and information techniques, we construct a robust\nmodel that detects a generated text for different types of bot. We find that\nthe generated texts tend to be more chaotic while literary works are more\ncomplex. We also demonstrate that the clustering of human texts results in\nfuzzier clusters in comparison to the more compact and well-separated clusters\nof bot-generated texts.\n","authors":["Vasilii Gromov","Quynh Nhu Dang"],"pdf_url":"https://arxiv.org/pdf/2311.11441v1.pdf","comment":"Accepted in Pattern Recognition and Machine Intelligence 2023. 8\n  pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.11435v1","updated":"2023-11-19T22:14:48Z","published":"2023-11-19T22:14:48Z","title":"Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis\n  of COVID-19 Vaccines in India","summary":"  In March 2020, the World Health Organisation declared COVID-19 a global\npandemic as it spread to nearly every country. By mid-2021, India had\nintroduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure\nsuccessful vaccination in a densely populated country like India, understanding\npublic sentiment was crucial. Social media, particularly Reddit with over 430\nmillion users, played a vital role in disseminating information. This study\nemploys data mining techniques to analyze Reddit data and gauge Indian\nsentiments towards COVID-19 vaccines. Using Python's Text Blob library,\ncomments are annotated to assess general sentiments. Results show that most\nReddit users in India expressed neutrality about vaccination, posing a\nchallenge for the Indian government's efforts to vaccinate a significant\nportion of the population.\n","authors":["Milind Gupta","Abhishek Kaushik"],"pdf_url":"https://arxiv.org/pdf/2311.11435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11415v1","updated":"2023-11-19T20:22:05Z","published":"2023-11-19T20:22:05Z","title":"A Security Risk Taxonomy for Large Language Models","summary":"  As large language models (LLMs) permeate more and more applications, an\nassessment of their associated security risks becomes increasingly necessary.\nThe potential for exploitation by malicious actors, ranging from disinformation\nto data breaches and reputation damage, is substantial. This paper addresses a\ngap in current research by focusing on the security risks posed by LLMs, which\nextends beyond the widely covered ethical and societal implications. Our work\nproposes a taxonomy of security risks along the user-model communication\npipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize\nthe attacks by target and attack type within a prompt-based interaction scheme.\nThe taxonomy is reinforced with specific attack examples to showcase the\nreal-world impact of these risks. Through this taxonomy, we aim to inform the\ndevelopment of robust and secure LLM applications, enhancing their safety and\ntrustworthiness.\n","authors":["Erik Derner","Kristina Batistič","Jan Zahálka","Robert Babuška"],"pdf_url":"https://arxiv.org/pdf/2311.11415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08365v2","updated":"2023-11-19T17:27:48Z","published":"2023-10-12T14:36:13Z","title":"From Large Language Models to Knowledge Graphs for Biomarker Discovery\n  in Cancer","summary":"  Domain experts often rely on most recent knowledge for apprehending and\ndisseminating specific biological processes that help them design strategies\nfor developing prevention and therapeutic decision-making in various disease\nscenarios. A challenging scenarios for artificial intelligence (AI) is using\nbiomedical data (e.g., texts, imaging, omics, and clinical) to provide\ndiagnosis and treatment recommendations for cancerous conditions.~Data and\nknowledge about biomedical entities like cancer, drugs, genes, proteins, and\ntheir mechanism is spread across structured (knowledge bases (KBs)) and\nunstructured (e.g., scientific articles) sources. A large-scale knowledge graph\n(KG) can be constructed by integrating and extracting facts about semantically\ninterrelated entities and relations. Such a KG not only allows exploration and\nquestion answering (QA) but also enables domain experts to deduce new\nknowledge. However, exploring and querying large-scale KGs is tedious for\nnon-domain users due to their lack of understanding of the data assets and\nsemantic technologies. In this paper, we develop a domain KG to leverage\ncancer-specific biomarker discovery and interactive QA. For this, we\nconstructed a domain ontology called OncoNet Ontology (ONO), which enables\nsemantic reasoning for validating gene-disease (different types of cancer)\nrelations. The KG is further enriched by harmonizing the ONO, metadata,\ncontrolled vocabularies, and biomedical concepts from scientific articles by\nemploying BioBERT- and SciBERT-based information extractors. Further, since the\nbiomedical domain is evolving, where new findings often replace old ones,\nwithout having access to up-to-date scientific findings, there is a high chance\nan AI system exhibits concept drift while providing diagnosis and treatment.\nTherefore, we fine-tune the KG using large language models (LLMs) based on more\nrecent articles and KBs.\n","authors":["Md. Rezaul Karim","Lina Molinas Comet","Md Shajalal","Oya Deniz Beyan","Dietrich Rebholz-Schuhmann","Stefan Decker"],"pdf_url":"https://arxiv.org/pdf/2310.08365v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04737"},{"id":"http://arxiv.org/abs/2311.11375v1","updated":"2023-11-19T16:53:35Z","published":"2023-11-19T16:53:35Z","title":"ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for\n  Improving ASR Robustness in Spoken Language Understanding","summary":"  Spoken language understanding (SLU) is a fundamental task in the\ntask-oriented dialogue systems. However, the inevitable errors from automatic\nspeech recognition (ASR) usually impair the understanding performance and lead\nto error propagation. Although there are some attempts to address this problem\nthrough contrastive learning, they (1) treat clean manual transcripts and ASR\ntranscripts equally without discrimination in fine-tuning; (2) neglect the fact\nthat the semantically similar pairs are still pushed away when applying\ncontrastive learning; (3) suffer from the problem of Kullback-Leibler (KL)\nvanishing. In this paper, we propose Mutual Learning and Large-Margin\nContrastive Learning (ML-LMCL), a novel framework for improving ASR robustness\nin SLU. Specifically, in fine-tuning, we apply mutual learning and train two\nSLU models on the manual transcripts and the ASR transcripts, respectively,\naiming to iteratively share knowledge between these two models. We also\nintroduce a distance polarization regularizer to avoid pushing away the\nintra-cluster pairs as much as possible. Moreover, we use a cyclical annealing\nschedule to mitigate KL vanishing issue. Experiments on three datasets show\nthat ML-LMCL outperforms existing models and achieves new state-of-the-art\nperformance.\n","authors":["Xuxin Cheng","Bowen Cao","Qichen Ye","Zhihong Zhu","Hongxiang Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2311.11375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06476v3","updated":"2023-11-19T15:43:58Z","published":"2023-02-08T09:44:51Z","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?","summary":"  Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.\n","authors":["Chengwei Qin","Aston Zhang","Zhuosheng Zhang","Jiaao Chen","Michihiro Yasunaga","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2302.06476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09889v2","updated":"2023-11-19T15:23:17Z","published":"2023-11-16T13:37:21Z","title":"Language Generation from Human Brain Activities","summary":"  Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.\n","authors":["Ziyi Ye","Qingyao Ai","Yiqun Liu","Min Zhang","Christina Lioma","Tuukka Ruotsalo"],"pdf_url":"https://arxiv.org/pdf/2311.09889v2.pdf","comment":"Preprint. Under Submission"},{"id":"http://arxiv.org/abs/2302.08143v3","updated":"2023-11-19T14:31:53Z","published":"2023-02-16T08:37:22Z","title":"Learning to Initialize: Can Meta Learning Improve Cross-task\n  Generalization in Prompt Tuning?","summary":"  Prompt tuning (PT) which only tunes the embeddings of an additional sequence\nof tokens per task, keeping the pre-trained language model (PLM) frozen, has\nshown remarkable performance in few-shot learning. Despite this, PT has been\nshown to rely heavily on good initialization of the prompt embeddings. In this\nwork, we study meta prompt tuning (MPT) to systematically explore how\nmeta-learning can help improve (if it can) cross-task generalization in PT\nthrough learning to initialize the prompt embeddings from other relevant tasks.\nWe empirically analyze a representative set of meta learning algorithms in a\nwide range of adaptation settings with different source/target task\nconfigurations on a large set of few-shot tasks. With extensive experiments and\nanalysis, we demonstrate the effectiveness of MPT. We find the improvement to\nbe significant particularly on classification tasks. For other kinds of tasks\nsuch as question answering, we observe that while MPT can outperform PT in most\ncases, it does not always outperform multi-task learning. We further provide an\nin-depth analysis from the perspective of task similarity.\n","authors":["Chengwei Qin","Qian Li","Ruochen Zhao","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2302.08143v3.pdf","comment":"ACL2023"},{"id":"http://arxiv.org/abs/2311.11331v1","updated":"2023-11-19T14:07:57Z","published":"2023-11-19T14:07:57Z","title":"Portuguese FAQ for Financial Services","summary":"  Scarcity of domain-specific data in the Portuguese financial domain has\ndisfavored the development of Natural Language Processing (NLP) applications.\nTo address this limitation, the present study advocates for the utilization of\nsynthetic data generated through data augmentation techniques. The\ninvestigation focuses on the augmentation of a dataset sourced from the Central\nBank of Brazil FAQ, employing techniques that vary in semantic similarity.\nSupervised and unsupervised tasks are conducted to evaluate the impact of\naugmented data on both low and high semantic similarity scenarios.\nAdditionally, the resultant dataset will be publicly disseminated on the\nHugging Face Datasets platform, thereby enhancing accessibility and fostering\nbroader engagement within the NLP research community.\n","authors":["Paulo Finardi","Wanderley M. Melo","Edgard D. Medeiros Neto","Alex F. Mansano","Pablo B. Costa","Vinicius F. Caridá"],"pdf_url":"https://arxiv.org/pdf/2311.11331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09886v3","updated":"2023-11-19T13:52:19Z","published":"2023-10-15T16:51:11Z","title":"Lifelong Sequence Generation with Dynamic Module Expansion and\n  Adaptation","summary":"  Lifelong sequence generation (LSG), a problem in continual learning, aims to\ncontinually train a model on a sequence of generation tasks to learn constantly\nemerging new generation patterns while avoiding the forgetting of previous\nknowledge. Existing LSG methods mainly focus on maintaining old knowledge while\npaying little attention to knowledge transfer across tasks. In contrast, humans\ncan better learn new tasks by leveraging previously acquired knowledge from\nsimilar tasks. Inspired by the learning paradigm of humans, we propose Dynamic\nModule Expansion and Adaptation (DMEA), which enables the model to dynamically\ndetermine the architecture for acquiring new knowledge based on task\ncorrelation and select the most similar previous tasks to facilitate adaptation\nto new tasks. In addition, as the learning process can easily be biased towards\nthe current task which might cause more severe forgetting of previously learned\nknowledge, we propose dynamic gradient scaling to balance the learning of the\ncurrent task and replayed tasks. With extensive experiments, we demonstrate\nthat DMEA can consistently outperform existing methods in different LSG\nsettings.\n","authors":["Chengwei Qin","Chen Chen","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2310.09886v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11732v2","updated":"2023-11-19T12:40:41Z","published":"2023-10-18T06:07:28Z","title":"Investigating Uncertainty Calibration of Aligned Language Models under\n  the Multiple-Choice Setting","summary":"  Despite the significant progress made in practical applications of aligned\nlanguage models (LMs), they tend to be overconfident in output answers compared\nto the corresponding pre-trained LMs. In this work, we systematically evaluate\nthe impact of the alignment process on logit-based uncertainty calibration of\nLMs under the multiple-choice setting. We first conduct a thoughtful empirical\nstudy on how aligned LMs differ in calibration from their pre-trained\ncounterparts. Experimental results reveal that there are two distinct\nuncertainties in LMs under the multiple-choice setting, which are responsible\nfor the answer decision and the format preference of the LMs, respectively.\nThen, we investigate the role of these two uncertainties on aligned LM's\ncalibration through fine-tuning in simple synthetic alignment schemes and\nconclude that one reason for aligned LMs' overconfidence is the conflation of\nthese two types of uncertainty. Furthermore, we examine the utility of common\npost-hoc calibration methods for aligned LMs and propose an easy-to-implement\nand sample-efficient method to calibrate aligned LMs. We hope our findings\ncould provide insights into the design of more reliable alignment processes for\nLMs.\n","authors":["Guande He","Peng Cui","Jianfei Chen","Wenbo Hu","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.11732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11301v1","updated":"2023-11-19T11:22:00Z","published":"2023-11-19T11:22:00Z","title":"CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies","summary":"  Various NLP tasks require a complex hierarchical structure over nodes, where\neach node is a cluster of items. Examples include generating entailment graphs,\nhierarchical cross-document coreference resolution, annotating event and\nsubevent relations, etc. To enable efficient annotation of such hierarchical\nstructures, we release CHAMP, an open source tool allowing to incrementally\nconstruct both clusters and hierarchy simultaneously over any type of texts.\nThis incremental approach significantly reduces annotation time compared to the\ncommon pairwise annotation approach and also guarantees maintaining\ntransitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a\nconsolidation mode, where an adjudicator can easily compare multiple cluster\nhierarchy annotations and resolve disagreements.\n","authors":["Arie Cattan","Tom Hope","Doug Downey","Roy Bar-Haim","Lilach Eden","Yoav Kantor","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2311.11301v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.11271v1","updated":"2023-11-19T08:54:47Z","published":"2023-11-19T08:54:47Z","title":"A Cross-Attention Augmented Model for Event-Triggered Context-Aware\n  Story Generation","summary":"  Despite recent advancements, existing story generation systems continue to\nencounter difficulties in effectively incorporating contextual and event\nfeatures, which greatly influence the quality of generated narratives. To\ntackle these challenges, we introduce a novel neural generation model, EtriCA,\nthat enhances the relevance and coherence of generated stories by employing a\ncross-attention mechanism to map context features onto event sequences through\nresidual mapping. This feature capturing mechanism enables our model to exploit\nlogical relationships between events more effectively during the story\ngeneration process. To further enhance our proposed model, we employ a\npost-training framework for knowledge enhancement (KeEtriCA) on a large-scale\nbook corpus. This allows EtriCA to adapt to a wider range of data samples. This\nresults in approximately 5\\% improvement in automatic metrics and over 10\\%\nimprovement in human evaluation. We conduct extensive experiments, including\ncomparisons with state-of-the-art (SOTA) baseline models, to evaluate the\nperformance of our framework on story generation. The experimental results,\nencompassing both automated metrics and human assessments, demonstrate the\nsuperiority of our model over existing state-of-the-art baselines. These\nresults underscore the effectiveness of our model in leveraging context and\nevent features to improve the quality of generated narratives.\n","authors":["Chen Tang","Tyler Loakman","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2311.11271v1.pdf","comment":"Submitted to CSL"},{"id":"http://arxiv.org/abs/2311.11268v1","updated":"2023-11-19T08:41:43Z","published":"2023-11-19T08:41:43Z","title":"Towards Real-World Writing Assistance: A Chinese Character Checking\n  Benchmark with Faked and Misspelled Characters","summary":"  Writing assistance is an application closely related to human life and is\nalso a fundamental Natural Language Processing (NLP) research field. Its aim is\nto improve the correctness and quality of input texts, with character checking\nbeing crucial in detecting and correcting wrong characters. From the\nperspective of the real world where handwriting occupies the vast majority,\ncharacters that humans get wrong include faked characters (i.e., untrue\ncharacters created due to writing errors) and misspelled characters (i.e., true\ncharacters used incorrectly due to spelling errors). However, existing datasets\nand related studies only focus on misspelled characters mainly caused by\nphonological or visual confusion, thereby ignoring faked characters which are\nmore common and difficult. To break through this dilemma, we present\nVisual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with\nfaked and misspelled Chinese characters. To the best of our knowledge,\nVisual-C$^3$ is the first real-world visual and the largest human-crafted\ndataset for the Chinese character checking scenario. Additionally, we also\npropose and evaluate novel baseline methods on Visual-C$^3$. Extensive\nempirical results and analyses show that Visual-C$^3$ is high-quality yet\nchallenging. The Visual-C$^3$ dataset and the baseline methods will be publicly\navailable to facilitate further research in the community.\n","authors":["Yinghui Li","Zishan Xu","Shaoshen Chen","Haojing Huang","Yangning Li","Yong Jiang","Zhongli Li","Qingyu Zhou","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2311.11268v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.11267v1","updated":"2023-11-19T08:40:01Z","published":"2023-11-19T08:40:01Z","title":"Rethinking Large Language Models in Mental Health Applications","summary":"  Large Language Models (LLMs) have become valuable assets in mental health,\nshowing promise in both classification tasks and counseling applications. This\npaper offers a perspective on using LLMs in mental health applications. It\ndiscusses the instability of generative models for prediction and the potential\nfor generating hallucinatory outputs, underscoring the need for ongoing audits\nand evaluations to maintain their reliability and dependability. The paper also\ndistinguishes between the often interchangeable terms ``explainability'' and\n``interpretability'', advocating for developing inherently interpretable\nmethods instead of relying on potentially hallucinated self-explanations\ngenerated by LLMs. Despite the advancements in LLMs, human counselors'\nempathetic understanding, nuanced interpretation, and contextual awareness\nremain irreplaceable in the sensitive and complex realm of mental health\ncounseling. The use of LLMs should be approached with a judicious and\nconsiderate mindset, viewing them as tools that complement human expertise\nrather than seeking to replace it.\n","authors":["Shaoxiong Ji","Tianlin Zhang","Kailai Yang","Sophia Ananiadou","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2311.11267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07989v2","updated":"2023-11-19T08:37:31Z","published":"2023-11-14T08:34:26Z","title":"A Survey on Language Models for Code","summary":"  In this work we systematically review the recent advancements in code\nprocessing with language models, covering 50+ models, 30+ evaluation tasks,\n150+ datasets, and 550 related works. We break down code processing models into\ngeneral language models represented by the GPT family and specialized models\nthat are specifically pretrained on code, often with tailored objectives. We\ndiscuss the relations and differences between these models, and highlight the\nhistorical transition of code modeling from statistical models and RNNs to\npretrained Transformers and LLMs, which is exactly the same course that had\nbeen taken by NLP. We also discuss code-specific features such as AST, CFG, and\nunit tests, along with their application in training code language models, and\nidentify key challenges and potential future directions in this domain. We keep\nthe survey open and updated on GitHub repository at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n","authors":["Ziyin Zhang","Chaoyu Chen","Bingchang Liu","Cong Liao","Zi Gong","Hang Yu","Jianguo Li","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07989v2.pdf","comment":"Repo is available at https://github.com/codefuse-ai/Awesome-Code-LLM.\n  V2 adds several new tasks, and collates dozens more benchmarks"},{"id":"http://arxiv.org/abs/2303.15445v2","updated":"2023-11-19T07:43:22Z","published":"2023-03-27T17:59:55Z","title":"IRFL: Image Recognition of Figurative Language","summary":"  Figures of speech such as metaphors, similes, and idioms are integral parts\nof human communication. They are ubiquitous in many forms of discourse,\nallowing people to convey complex, abstract ideas and evoke emotion. As\nfigurative forms are often conveyed through multiple modalities (e.g., both\ntext and images), understanding multimodal figurative language is an important\nAI challenge, weaving together profound vision, language, commonsense and\ncultural knowledge.\n  In this work, we develop the Image Recognition of Figurative Language (IRFL)\ndataset. We leverage human annotation and an automatic pipeline we created to\ngenerate a multimodal dataset, and introduce two novel tasks as a benchmark for\nmultimodal figurative language understanding. We experimented with\nstate-of-the-art vision and language models and found that the best (22%)\nperformed substantially worse than humans (97%). We release our dataset,\nbenchmark, and code, in hopes of driving the development of models that can\nbetter understand figurative language.\n","authors":["Ron Yosef","Yonatan Bitton","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2303.15445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11229v1","updated":"2023-11-19T05:00:39Z","published":"2023-11-19T05:00:39Z","title":"Causal ATE Mitigates Unintended Bias in Controlled Text Generation","summary":"  We study attribute control in language models through the method of Causal\nAverage Treatment Effect (Causal ATE). Existing methods for the attribute\ncontrol task in Language Models (LMs) check for the co-occurrence of words in a\nsentence with the attribute of interest, and control for them. However,\nspurious correlation of the words with the attribute in the training dataset,\ncan cause models to hallucinate the presence of the attribute when presented\nwith the spurious correlate during inference. We show that the simple\nperturbation-based method of Causal ATE removes this unintended effect.\nAdditionally, we offer a theoretical foundation for investigating Causal ATE in\nthe classification task, and prove that it reduces the number of false\npositives -- thereby mitigating the issue of unintended bias. Specifically, we\nground it in the problem of toxicity mitigation, where a significant challenge\nlies in the inadvertent bias that often emerges towards protected groups post\ndetoxification. We show that this unintended bias can be solved by the use of\nthe Causal ATE metric.\n","authors":["Rahul Madhavan","Kahini Wadhawan"],"pdf_url":"https://arxiv.org/pdf/2311.11229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11215v1","updated":"2023-11-19T03:43:42Z","published":"2023-11-19T03:43:42Z","title":"SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data","summary":"  Effective cyber threat recognition and prevention demand comprehensible\nforecasting systems, as prior approaches commonly offer limited and,\nultimately, unconvincing information. We introduce Simplified Plaintext\nLanguage (SPLAIN), a natural language generator that converts warning data into\nuser-friendly cyber threat explanations. SPLAIN is designed to generate clear,\nactionable outputs, incorporating hierarchically organized explanatory details\nabout input data and system functionality. Given the inputs of individual\nsensor-induced forecasting signals and an overall warning from a fusion module,\nSPLAIN queries each signal for information on contributing sensors and data\nsignals. This collected data is processed into a coherent English explanation,\nencompassing forecasting, sensing, and data elements for user review. SPLAIN's\ntemplate-based approach ensures consistent warning structure and vocabulary.\nSPLAIN's hierarchical output structure allows each threat and its components to\nbe expanded to reveal underlying explanations on demand. Our conclusions\nemphasize the need for designers to specify the \"how\" and \"why\" behind cyber\nwarnings, advocate for simple structured templates in generating consistent\nexplanations, and recognize that direct causal links in Machine Learning\napproaches may not always be identifiable, requiring some explanations to focus\non general methodologies, such as model and training data.\n","authors":["Vera A. Kazakova","Jena D. Hwang","Bonnie J. Dorr","Yorick Wilks","J. Blake Gage","Alex Memory","Mark A. Clark"],"pdf_url":"https://arxiv.org/pdf/2311.11215v1.pdf","comment":"Presented at FLAIRS-2019 as poster (see ancillary files)"},{"id":"http://arxiv.org/abs/2311.11202v1","updated":"2023-11-19T02:34:12Z","published":"2023-11-19T02:34:12Z","title":"Unmasking and Improving Data Credibility: A Study with Datasets for\n  Training Harmless Language Models","summary":"  Language models have shown promise in various tasks but can be affected by\nundesired data during training, fine-tuning, or alignment. For example, if some\nunsafe conversations are wrongly annotated as safe ones, the model fine-tuned\non these samples may be harmful. Therefore, the correctness of annotations,\ni.e., the credibility of the dataset, is important. This study focuses on the\ncredibility of real-world datasets, including the popular benchmarks Jigsaw\nCivil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that\ncan be used for training a harmless language model. Given the cost and\ndifficulty of cleaning these datasets by humans, we introduce a systematic\nframework for evaluating the credibility of datasets, identifying label errors,\nand evaluating the influence of noisy labels in the curated language data,\nspecifically focusing on unsafe comments and conversation classification. With\nthe framework, we find and fix an average of 6.16% label errors in 11 datasets\nconstructed from the above benchmarks. The data credibility and downstream\nlearning performance can be remarkably improved by directly fixing label\nerrors, indicating the significance of cleaning existing real-world datasets.\nOpen-source: https://github.com/Docta-ai/docta.\n","authors":["Zhaowei Zhu","Jialu Wang","Hao Cheng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.11202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07723v2","updated":"2023-11-19T01:33:53Z","published":"2023-11-13T20:07:36Z","title":"Generalization Analogies: A Testbed for Generalizing AI Oversight to\n  Hard-To-Measure Domains","summary":"  As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENeralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n","authors":["Joshua Clymer","Garrett Baker","Rohan Subramani","Sam Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07723v2.pdf","comment":"Code: https://github.com/Joshuaclymer/GENIES Website:\n  https://joshuaclymer.github.io/generalization-analogies-website/"},{"id":"http://arxiv.org/abs/2305.14864v2","updated":"2023-11-19T01:14:34Z","published":"2023-05-24T08:18:35Z","title":"How To Train Your (Compressed) Large Language Model","summary":"  With the increase in the size of large language models (LLMs), we need\ncompression methods that can reduce the model size while preserving the\ngenerality and zero-shot promptability of the model. This goal is more\nambitious than the typical compression setup, which reduces the model's size at\nthe expense of specializing it to a specific end-task. To study this, we\ndevelop a task-agnostic compression pipeline with a large-scale evaluation\ncomprising language modeling perplexity and 12 zero-shot end-tasks. Our results\nshow that a simple layer-wise pruning followed by continued language model\npretraining matches or outperforms three existing state-of-the-art baselines\nwhile being 1.5x more computationally efficient. However, unlike typical\ntask-specialized compression, our best-compressed model significantly\nunderperforms a similar-sized model trained from scratch. We posit the\nhalf-sized pretrained model as an upper bound for task-agnostic compression and\ncall for future work to bridge this gap under a reasonable token budget. Our\nfindings highlight the inadequacy of existing compression methods for LLMs and\nestablish a requirement for new methods that preserve a model's generality and\nzero-shot promptability under compression. We release our code and evaluation\nsetup to facilitate reproducibility and help iterate on method design.\n","authors":["Ananya Harsh Jha","Tom Sherborne","Evan Pete Walsh","Dirk Groeneveld","Emma Strubell","Iz Beltagy"],"pdf_url":"https://arxiv.org/pdf/2305.14864v2.pdf","comment":"13 pages, 6 figures, 5 tables"}]},"2023-11-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.04666v2","updated":"2023-11-18T22:14:48Z","published":"2023-11-08T13:13:23Z","title":"Pre-training LLMs using human-like development data corpus","summary":"  Pre-trained Large Language Models (LLMs) have shown success in a diverse set\nof language inference and understanding tasks. The pre-training stage of LLMs\nlooks at a large corpus of raw textual data. The BabyLM shared task compares\nLLM pre-training to human language acquisition, where the number of tokens seen\nby 13-year-old kids is magnitudes smaller than the number of tokens seen by\nLLMs. In this work, we pre-train and evaluate LLMs on their ability to learn\ncontextual word representations using roughly the same number of tokens as seen\nby children. We provide a strong set of baselines; with different\narchitectures, evaluation of changes in performance across epochs, and reported\npre-training metrics for the strict small and strict tracks of the task. We\nalso try to loosely replicate the RoBERTa baseline given by the task organizers\nto observe the training robustness to hyperparameter selection and\nreplicability. We provide the submission details to the strict and strict-small\ntracks in this report.\n","authors":["Khushi Bhardwaj","Raj Sanjay Shah","Sashank Varma"],"pdf_url":"https://arxiv.org/pdf/2311.04666v2.pdf","comment":"CoNLL and CMCL 2023"},{"id":"http://arxiv.org/abs/2302.03169v3","updated":"2023-11-18T21:33:01Z","published":"2023-02-06T23:57:56Z","title":"Data Selection for Language Models via Importance Resampling","summary":"  Selecting a suitable pretraining dataset is crucial for both general-domain\n(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We\nformalize this problem as selecting a subset of a large raw unlabeled dataset\nto match a desired target distribution given unlabeled target samples. Due to\nthe scale and dimensionality of the raw text data, existing methods use simple\nheuristics or require human experts to manually curate data. Instead, we extend\nthe classic importance resampling approach used in low-dimensions for LM data\nselection. We propose Data Selection with Importance Resampling (DSIR), an\nefficient and scalable framework that estimates importance weights in a reduced\nfeature space for tractability and selects data with importance resampling\naccording to these weights. We instantiate the DSIR framework with hashed\nn-gram features for efficiency, enabling the selection of 100M documents from\nthe full Pile dataset in 4.5 hours. To measure whether hashed n-gram features\npreserve the aspects of the data that are relevant to the target, we define KL\nreduction, a data metric that measures the proximity between the selected\npretraining data and the target on some feature space. Across 8 data selection\nmethods (including expert selection), KL reduction on hashed n-gram features\nhighly correlates with average downstream accuracy (r=0.82). When selecting\ndata for continued pretraining on a specific domain, DSIR performs comparably\nto expert curation across 8 target distributions. When pretraining\ngeneral-domain models (target is Wikipedia and books), DSIR improves over\nrandom selection and heuristic filtering baselines by 2-2.5% on the GLUE\nbenchmark. Code is available at https://github.com/p-lambda/dsir.\n","authors":["Sang Michael Xie","Shibani Santurkar","Tengyu Ma","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2302.03169v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.11161v1","updated":"2023-11-18T20:32:59Z","published":"2023-11-18T20:32:59Z","title":"Experts-in-the-Loop: Establishing an Effective Workflow in Crafting\n  Privacy Q&A","summary":"  Privacy policies play a vital role in safeguarding user privacy as legal\njurisdictions worldwide emphasize the need for transparent data processing.\nWhile the suitability of privacy policies to enhance transparency has been\ncritically discussed, employing conversational AI systems presents unique\nchallenges in informing users effectively. In this position paper, we propose a\ndynamic workflow for transforming privacy policies into privacy\nquestion-and-answer (Q&A) pairs to make privacy policies easily accessible\nthrough conversational AI. Thereby, we facilitate interdisciplinary\ncollaboration among legal experts and conversation designers, while also\nconsidering the utilization of large language models' generative capabilities\nand addressing associated challenges. Our proposed workflow underscores\ncontinuous improvement and monitoring throughout the construction of privacy\nQ&As, advocating for comprehensive review and refinement through an\nexperts-in-the-loop approach.\n","authors":["Zahra Kolagar","Anna Katharina Leschanowsky","Birgit Popp"],"pdf_url":"https://arxiv.org/pdf/2311.11161v1.pdf","comment":"Position paper presented at CONVERSATIONS 2023 - the 7th\n  International Workshop on Chatbot Research and Design, hosted by the\n  University of Oslo, Norway, November 22-23, 2023"},{"id":"http://arxiv.org/abs/2311.11159v1","updated":"2023-11-18T20:22:44Z","published":"2023-11-18T20:22:44Z","title":"Evaluating the Inclusiveness of Artificial Intelligence Software in\n  Enhancing Project Management Efficiency -- A Review","summary":"  The rise of advanced technology in project management (PM) highlights a\ncrucial need for inclusiveness. This work examines the enhancement of both\ninclusivity and efficiency in PM through technological integration, focusing on\ndefining and measuring inclusiveness. This approach illuminates how\ninclusivity-centered technology can significantly elevate project outcomes. The\nresearch navigates through the challenges of achieving inclusivity, mainly\nbiases in learning databases and the design process of these technologies,\nassessment of transformative potential of these technologies, particularly in\nautomating tasks like data collection and analysis, thus enabling managers to\nprioritize human-centric aspects of projects. However, the integration of such\ntechnology transcends efficiency, indicating a paradigm shift in understanding\ntheir societal roles. This shift necessitates a new approach in the development\nof these systems to prevent perpetuating social inequalities. We proposed a\nmethodology involving criteria development for evaluating the inclusiveness and\neffectiveness of these technologies. This methodical approach is vital to\ncomprehensively address the challenges and limitations inherent in these\nsystems. Emphasizing the importance of inclusivity, the study advocates for a\nbalance between technological advancement and ethical considerations, calling\nfor a holistic understanding and regulation. In conclusion, the paper\nunderscores that while these technologies can significantly improve outcomes,\ntheir mindful integration, ensuring inclusivity, is paramount. This exploration\ninto the ethical and practical aspects of technology in PM contributes to a\nmore informed and balanced approach within the field.\n","authors":["Vasileios Alevizos","Ilias Georgousis","Akebu Simasiku","Sotiria Karypidou","Antonis Messinis"],"pdf_url":"https://arxiv.org/pdf/2311.11159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07772v3","updated":"2023-11-18T19:58:27Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n","authors":["Gilad Deutch","Nadav Magar","Tomer Bar Natan","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11142v1","updated":"2023-11-18T18:36:16Z","published":"2023-11-18T18:36:16Z","title":"Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language","summary":"  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n","authors":["Fatema Tuj Johora Faria","Mukaffi Bin Moin","Ahmed Al Wase","Mehidi Ahmmed","Md. Rabius Sani","Tashreef Muhammad"],"pdf_url":"https://arxiv.org/pdf/2311.11142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14360v3","updated":"2023-11-18T18:10:34Z","published":"2023-10-22T17:03:56Z","title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding\n  address parsing techniques","summary":"  The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.\n","authors":["Zhengcong Yin","Diya Li","Daniel W. Goldberg"],"pdf_url":"https://arxiv.org/pdf/2310.14360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11135v1","updated":"2023-11-18T18:10:02Z","published":"2023-11-18T18:10:02Z","title":"A Principled Framework for Knowledge-enhanced Large Language Model","summary":"  Large Language Models (LLMs) are versatile, yet they often falter in tasks\nrequiring deep and reliable reasoning due to issues like hallucinations,\nlimiting their applicability in critical scenarios. This paper introduces a\nrigorously designed framework for creating LLMs that effectively anchor\nknowledge and employ a closed-loop reasoning process, enhancing their\ncapability for in-depth analysis. We dissect the framework to illustrate the\ncontribution of each component to the LLMs' performance, offering a theoretical\nassurance of improved reasoning under well-defined assumptions.\n","authors":["Saizhuo Wang","Zhihan Liu","Zhaoran Wang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2311.11135v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2201.01140v4","updated":"2023-11-18T17:20:23Z","published":"2022-01-04T14:05:49Z","title":"Predicting Influenza A Viral Host Using PSSM and Word Embeddings","summary":"  The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.\n","authors":["Yanhua Xu","Dominik Wojtczak"],"pdf_url":"https://arxiv.org/pdf/2201.01140v4.pdf","comment":"Accepted for publication at CIBCB 2021. V1: accepted version + minor\n  correction to table 1; V2: corrected a minor typo; V3: update the formula of\n  error rate; V4: replacing 'nested cv' with 'nested k-fold cv' for better\n  clarity"},{"id":"http://arxiv.org/abs/2311.11123v1","updated":"2023-11-18T17:11:12Z","published":"2023-11-18T17:11:12Z","title":"(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for\n  Evolving LLM APIs","summary":"  Large Language Models (LLMs) are increasingly integrated into software\napplications. Downstream application developers often access LLMs through APIs\nprovided as a service. However, LLM APIs are often updated silently and\nscheduled to be deprecated, forcing users to continuously adapt to evolving\nmodels. This can cause performance regression and affect prompt design choices,\nas evidenced by our case study on toxicity detection. Based on our case study,\nwe emphasize the need for and re-examine the concept of regression testing for\nevolving LLM APIs. We argue that regression testing LLMs requires fundamental\nchanges to traditional testing approaches, due to different correctness\nnotions, prompting brittleness, and non-determinism in LLM APIs.\n","authors":["Wanqin Ma","Chenyang Yang","Christian Kästner"],"pdf_url":"https://arxiv.org/pdf/2311.11123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19727v2","updated":"2023-11-18T17:10:29Z","published":"2023-10-30T16:53:11Z","title":"Generating Medical Prescriptions with Conditional Transformer","summary":"  Access to real-world medication prescriptions is essential for medical\nresearch and healthcare quality improvement. However, access to real medication\nprescriptions is often limited due to the sensitive nature of the information\nexpressed. Additionally, manually labelling these instructions for training and\nfine-tuning Natural Language Processing (NLP) models can be tedious and\nexpensive. We introduce a novel task-specific model architecture,\nLabel-To-Text-Transformer (\\textbf{LT3}), tailored to generate synthetic\nmedication prescriptions based on provided labels, such as a vocabulary list of\nmedications and their attributes. LT3 is trained on a set of around 2K lines of\nmedication prescriptions extracted from the MIMIC-III database, allowing the\nmodel to produce valuable synthetic medication prescriptions. We evaluate LT3's\nperformance by contrasting it with a state-of-the-art Pre-trained Language\nModel (PLM), T5, analysing the quality and diversity of generated texts. We\ndeploy the generated synthetic data to train the SpacyNER model for the Named\nEntity Recognition (NER) task over the n2c2-2018 dataset. The experiments show\nthat the model trained on synthetic data can achieve a 96-98\\% F1 score at\nLabel Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and\ndata will be shared at\n\\url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}\n","authors":["Samuel Belkadi","Nicolo Micheletti","Lifeng Han","Warren Del-Pinto","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2310.19727v2.pdf","comment":"Accepted to: Workshop on Synthetic Data Generation with Generative AI\n  (SyntheticData4ML Workshop) at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.11116v1","updated":"2023-11-18T16:35:55Z","published":"2023-11-18T16:35:55Z","title":"Utilizing Speech Emotion Recognition and Recommender Systems for\n  Negative Emotion Handling in Therapy Chatbots","summary":"  Emotional well-being significantly influences mental health and overall\nquality of life. As therapy chatbots become increasingly prevalent, their\nability to comprehend and respond empathetically to users' emotions remains\nlimited. This paper addresses this limitation by proposing an approach to\nenhance therapy chatbots with auditory perception, enabling them to understand\nusers' feelings and provide human-like empathy. The proposed method\nincorporates speech emotion recognition (SER) techniques using Convolutional\nNeural Network (CNN) models and the ShEMO dataset to accurately detect and\nclassify negative emotions, including anger, fear, and sadness. The SER model\nachieves a validation accuracy of 88%, demonstrating its effectiveness in\nrecognizing emotional states from speech signals. Furthermore, a recommender\nsystem is developed, leveraging the SER model's output to generate personalized\nrecommendations for managing negative emotions, for which a new bilingual\ndataset was generated as well since there is no such dataset available for this\ntask. The recommender model achieves an accuracy of 98% by employing a\ncombination of global vectors for word representation (GloVe) and LSTM models.\nTo provide a more immersive and empathetic user experience, a text-to-speech\nmodel called GlowTTS is integrated, enabling the therapy chatbot to audibly\ncommunicate the generated recommendations to users in both English and Persian.\nThe proposed approach offers promising potential to enhance therapy chatbots by\nproviding them with the ability to recognize and respond to users' emotions,\nultimately improving the delivery of mental health support for both English and\nPersian-speaking users.\n","authors":["Farideh Majidi","Marzieh Bahrami"],"pdf_url":"https://arxiv.org/pdf/2311.11116v1.pdf","comment":"Accepted at the First National Conference of Artificial Intelligence\n  and Software Engineering"},{"id":"http://arxiv.org/abs/2311.11103v1","updated":"2023-11-18T15:35:36Z","published":"2023-11-18T15:35:36Z","title":"Responsible AI Considerations in Text Summarization Research: A Review\n  of Current Practices","summary":"  AI and NLP publication venues have increasingly encouraged researchers to\nreflect on possible ethical considerations, adverse impacts, and other\nresponsible AI issues their work might engender. However, for specific NLP\ntasks our understanding of how prevalent such issues are, or when and why these\nissues are likely to arise, remains limited. Focusing on text summarization --\na common NLP task largely overlooked by the responsible AI community -- we\nexamine research and reporting practices in the current literature. We conduct\na multi-round qualitative analysis of 333 summarization papers from the ACL\nAnthology published between 2020-2022. We focus on how, which, and when\nresponsible AI issues are covered, which relevant stakeholders are considered,\nand mismatches between stated and realized research goals. We also discuss\ncurrent evaluation practices and consider how authors discuss the limitations\nof both prior work and their own work. Overall, we find that relatively few\npapers engage with possible stakeholders or contexts of use, which limits their\nconsideration of potential downstream adverse impacts or other responsible AI\nissues. Based on our findings, we make recommendations on concrete practices\nand research directions.\n","authors":["Yu Lu Liu","Meng Cao","Su Lin Blodgett","Jackie Chi Kit Cheung","Alexandra Olteanu","Adam Trischler"],"pdf_url":"https://arxiv.org/pdf/2311.11103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11097v1","updated":"2023-11-18T14:52:26Z","published":"2023-11-18T14:52:26Z","title":"Radiology Report Generation Using Transformers Conditioned with\n  Non-imaging Data","summary":"  Medical image interpretation is central to most clinical applications such as\ndisease diagnosis, treatment planning, and prognostication. In clinical\npractice, radiologists examine medical images and manually compile their\nfindings into reports, which can be a time-consuming process. Automated\napproaches to radiology report generation, therefore, can reduce radiologist\nworkload and improve efficiency in the clinical pathway. While recent\ndeep-learning approaches for automated report generation from medical images\nhave seen some success, most studies have relied on image-derived features\nalone, ignoring non-imaging patient data. Although a few studies have included\nthe word-level contexts along with the image, the use of patient demographics\nis still unexplored. This paper proposes a novel multi-modal transformer\nnetwork that integrates chest x-ray (CXR) images and associated patient\ndemographic information, to synthesise patient-specific radiology reports. The\nproposed network uses a convolutional neural network to extract visual features\nfrom CXRs and a transformer-based encoder-decoder network that combines the\nvisual features with semantic text embeddings of patient demographic\ninformation, to synthesise full-text radiology reports. Data from two public\ndatabases were used to train and evaluate the proposed approach. CXRs and\nreports were extracted from the MIMIC-CXR database and combined with\ncorresponding patients' data MIMIC-IV. Based on the evaluation metrics used\nincluding patient demographic information was found to improve the quality of\nreports generated using the proposed approach, relative to a baseline network\ntrained using CXRs alone. The proposed approach shows potential for enhancing\nradiology report generation by leveraging rich patient metadata and combining\nsemantic text embeddings derived thereof, with medical image-derived visual\nfeatures.\n","authors":["Nurbanu Aksoy","Nishant Ravikumar","Alejandro F Frangi"],"pdf_url":"https://arxiv.org/pdf/2311.11097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11090v1","updated":"2023-11-18T14:37:53Z","published":"2023-11-18T14:37:53Z","title":"Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report\n  Generation","summary":"  Image-to-text radiology report generation aims to automatically produce\nradiology reports that describe the findings in medical images. Most existing\nmethods focus solely on the image data, disregarding the other patient\ninformation accessible to radiologists. In this paper, we present a novel\nmulti-modal deep neural network framework for generating chest X-rays reports\nby integrating structured patient data, such as vital signs and symptoms,\nalongside unstructured clinical notes.We introduce a conditioned\ncross-multi-head attention module to fuse these heterogeneous data modalities,\nbridging the semantic gap between visual and textual data. Experiments\ndemonstrate substantial improvements from using additional modalities compared\nto relying on images alone. Notably, our model achieves the highest reported\nperformance on the ROUGE-L metric compared to relevant state-of-the-art models\nin the literature. Furthermore, we employed both human evaluation and clinical\nsemantic similarity measurement alongside word-overlap metrics to improve the\ndepth of quantitative analysis. A human evaluation, conducted by a\nboard-certified radiologist, confirms the model's accuracy in identifying\nhigh-level findings, however, it also highlights that more improvement is\nneeded to capture nuanced details and clinical context.\n","authors":["Nurbanu Aksoy","Serge Sharoff","Selcuk Baser","Nishant Ravikumar","Alejandro F Frangi"],"pdf_url":"https://arxiv.org/pdf/2311.11090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11088v1","updated":"2023-11-18T14:35:26Z","published":"2023-11-18T14:35:26Z","title":"Combining EEG and NLP Features for Predicting Students' Lecture\n  Comprehension using Ensemble Classification","summary":"  Electroencephalography (EEG) and Natural Language Processing (NLP) can be\napplied for education to measure students' comprehension in classroom lectures;\ncurrently, the two measures have been used separately. In this work, we propose\na classification framework for predicting students' lecture comprehension in\ntwo tasks: (i) students' confusion after listening to the simulated lecture and\n(ii) the correctness of students' responses to the post-lecture assessment. The\nproposed framework includes EEG and NLP feature extraction, processing, and\nclassification. EEG and NLP features are extracted to construct integrated\nfeatures obtained from recorded EEG signals and sentence-level syntactic\nanalysis, which provide information about specific biomarkers and sentence\nstructures. An ensemble stacking classification method -- a combination of\nmultiple individual models that produces an enhanced predictive model -- is\nstudied to learn from the features to make predictions accurately. Furthermore,\nwe also utilized subjective confusion ratings as another integrated feature to\nenhance classification performance. By doing so, experiment results show that\nthis framework performs better than the baselines, which achieved F1 up to 0.65\nfor predicting confusion and 0.78 for predicting correctness, highlighting that\nutilizing this has helped improve the classification performance.\n","authors":["Phantharach Natnithikarat","Theerawit Wilaiprasitporn","Supavit Kongwudhikunakorn"],"pdf_url":"https://arxiv.org/pdf/2311.11088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10057v2","updated":"2023-11-18T14:14:40Z","published":"2023-11-16T17:52:21Z","title":"The Song Describer Dataset: a Corpus of Audio Captions for\n  Music-and-Language Evaluation","summary":"  We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of\nhigh-quality audio-caption pairs, designed for the evaluation of\nmusic-and-language models. The dataset consists of 1.1k human-written natural\nlanguage descriptions of 706 music recordings, all publicly accessible and\nreleased under Creative Common licenses. To showcase the use of our dataset, we\nbenchmark popular models on three key music-and-language tasks (music\ncaptioning, text-to-music generation and music-language retrieval). Our\nexperiments highlight the importance of cross-dataset evaluation and offer\ninsights into how researchers can use SDD to gain a broader understanding of\nmodel performance.\n","authors":["Ilaria Manco","Benno Weck","SeungHeon Doh","Minz Won","Yixiao Zhang","Dmitry Bodganov","Yusong Wu","Ke Chen","Philip Tovstogan","Emmanouil Benetos","Elio Quinton","György Fazekas","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2311.10057v2.pdf","comment":"Accepted to NeurIPS 2023 Workshop on Machine Learning for Audio"},{"id":"http://arxiv.org/abs/2311.11077v1","updated":"2023-11-18T13:53:26Z","published":"2023-11-18T13:53:26Z","title":"Adapters: A Unified Library for Parameter-Efficient and Modular Transfer\n  Learning","summary":"  We introduce Adapters, an open-source library that unifies\nparameter-efficient and modular transfer learning in large language models. By\nintegrating 10 diverse adapter methods into a unified interface, Adapters\noffers ease of use and flexible configuration. Our library allows researchers\nand practitioners to leverage adapter modularity through composition blocks,\nenabling the design of complex adapter setups. We demonstrate the library's\nefficacy by evaluating its performance against full fine-tuning on various NLP\ntasks. Adapters provides a powerful tool for addressing the challenges of\nconventional fine-tuning paradigms and promoting more efficient and modular\ntransfer learning. The library is available via https://adapterhub.ml/adapters.\n","authors":["Clifton Poth","Hannah Sterz","Indraneil Paul","Sukannya Purkayastha","Leon Engländer","Timo Imhof","Ivan Vulić","Sebastian Ruder","Iryna Gurevych","Jonas Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2311.11077v1.pdf","comment":"EMNLP 2023: Systems Demonstrations"}]}}